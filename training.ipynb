{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70829a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (1.25.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/python/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/python/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/python/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/python/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/python/3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/python/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.3.1)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m128.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, fsspec, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/python/3.10/lib/python3.10/site-packages/~yarrow-14.0.2.dist-info'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/python/3.10/lib/python3.10/site-packages/~yarrow'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/python/3.10/lib/python3.10/site-packages/~sspec-2025.3.2.dist-info'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/python/3.10/lib/python3.10/site-packages/~sspec'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 19.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 huggingface-hub-0.30.2 multiprocess-0.70.16 pyarrow-19.0.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting FlagEmbedding[finetune]\n",
      "  Downloading FlagEmbedding-1.3.4.tar.gz (163 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch>=1.6.0 (from FlagEmbedding[finetune])\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting transformers>=4.44.2 (from FlagEmbedding[finetune])\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: datasets>=2.19.0 in /opt/python/3.10/lib/python3.10/site-packages (from FlagEmbedding[finetune]) (3.5.0)\n",
      "Collecting accelerate>=0.20.1 (from FlagEmbedding[finetune])\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence_transformers (from FlagEmbedding[finetune])\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting peft (from FlagEmbedding[finetune])\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting ir-datasets (from FlagEmbedding[finetune])\n",
      "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from FlagEmbedding[finetune])\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /opt/python/3.10/lib/python3.10/site-packages (from FlagEmbedding[finetune]) (3.20.3)\n",
      "Collecting deepspeed (from FlagEmbedding[finetune])\n",
      "  Downloading deepspeed-0.16.6.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flash-attn (from FlagEmbedding[finetune])\n",
      "  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/python/3.10/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/python/3.10/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/python/3.10/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/var/tmp/pip-build-env-7jc66amq/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 334, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m   File \"/var/tmp/pip-build-env-7jc66amq/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 304, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/var/tmp/pip-build-env-7jc66amq/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 522, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/var/tmp/pip-build-env-7jc66amq/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 320, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 22, in <module>\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/python/3.10/lib/python3.10/site-packages (from jsonlines) (25.3.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: jsonlines\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed jsonlines-4.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/python/3.10/lib/python3.10/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/3.10/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/python/3.10/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/python/3.10/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/python/3.10/lib/python3.10/site-packages (from accelerate) (0.30.2)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/python/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in /opt/python/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/python/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/python/3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/python/3.10/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=2.0.0->accelerate)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=2.0.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/3.10/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/python/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m170.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/python/3.10/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, accelerate\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/python/3.10/share/man/man1/isympy.1'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install datasets\n",
    "!pip install FlagEmbedding[finetune]\n",
    "!pip install jsonlines\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa292f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c038f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, arrow_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import jsonlines\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c071e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset(\"castorini/wura\", \"yor\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "data = load_dataset(\"castorini/wura\", \"ibo\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73660d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wura(dataset):\n",
    "    if not isinstance(dataset, arrow_dataset.Dataset):\n",
    "        raise ValueError(f\"The parameter `dataset` only accepts `arrow_dataset.Dataset` objects. Got {type(dataset)} instead.\")\n",
    "\n",
    "    expected_columns = {\"headline\", \"content\", \"category\", \"url\"}\n",
    "    missing_columns = expected_columns.difference(set(dataset.features))\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The dataset must contain all of the following features: {expected_columns}. Missing features: {missing_columns}\")\n",
    "\n",
    "    domain_counts = {}\n",
    "    for row in dataset:\n",
    "        domain = extract_domain_name(row[\"url\"])\n",
    "        domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "    invalid_domains = {\n",
    "        \"jw.org\" # Has really weird links, for example:  https://www.jw.org/yo/elerii-jehofa/kan-si-wa/venezuela/, https://www.jw.org/yo/elerii-jehofa/kan-si-wa/tonga/, https://www.jw.org/yo/elerii-jehofa/kan-si-wa/taiwan/ all have the title \"Kan Si Wa\"\n",
    "    }\n",
    "\n",
    "    is_headline_valid = lambda value: len((value or \" \").split()) > 1\n",
    "    is_url_valid = lambda value: len((value or \" \").strip()) > 5\n",
    "    is_domain_valid = lambda value: domain_counts[value] > 10 and not value in invalid_domains # If the domain does not appear enough times that is a sign that the site is not committed to publishing in the language. So it is probably a weird url or the English was translated using Google translate e.g. https://downloadfacetime.com/facetime/facetime-for-ipad/\n",
    "    is_text_valid = lambda value: len((value or \" \").strip().split()) > 30\n",
    "\n",
    "    data = []\n",
    "    for row in dataset:\n",
    "        if not (is_headline_valid(row[\"headline\"]) \\\n",
    "                and is_url_valid(row[\"url\"]) \\\n",
    "                and is_domain_valid(extract_domain_name(row[\"url\"]))):\n",
    "            continue\n",
    "\n",
    "        data.append({\n",
    "            \"title\": row[\"headline\"],\n",
    "            \"url\": row[\"url\"].strip(\"/\") + \"/\", \"text\": row[\"content\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        })\n",
    "\n",
    "    wura_df = pd.DataFrame(data)\n",
    "    return wura_df\n",
    "\n",
    "\n",
    "def split_wura_validation_all_langs():\n",
    "    languages = [\"yor\", \"igbo\", \"hau\"]\n",
    "    dfs = {}\n",
    "    for lang in languages:\n",
    "        wura_lang = \"ibo\" if lang == \"igbo\" else lang\n",
    "        dataset = load_dataset(\"castorini/wura\", wura_lang, level=\"document\", trust_remote_code=True)\n",
    "        validation_data = dataset.get(\"validation\")\n",
    "        if not validation_data:\n",
    "            raise ValueError(f\"Dataset {wura_lang} does not have a validation split. Only found {dataset.keys()} splits.\")\n",
    "        lang_df = prepare_wura(validation_data)\n",
    "        lang_df.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "        eval_df, test_df = train_test_split(lang_df, test_size=0.4, random_state=SEED, shuffle=True)\n",
    "        eval_df.to_json(f\"{lang}_eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "        test_df.to_json(f\"{lang}_test_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "        dfs[lang] = {\n",
    "            \"eval\": eval_df,\n",
    "            \"test\": test_df\n",
    "        }\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548cb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"bbc.com\"\n",
    "# df[df[\"domain_name\"] == domain].url.tolist()\n",
    "# df[df[\"domain_name\"] == domain].head(15).pos.tolist()\n",
    "# df[df[\"domain_name\"] == domain].head(15).url.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed474bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def extract_domain_name(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        netloc = str(parsed_url.netloc)\n",
    "        return netloc.strip(\"www.\")\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c59a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wura_remove_validation_rows(df, wura_ds):\n",
    "    \"\"\"Checks for rows in df that exist in wura_ds, using the url, then drops them\"\"\"\n",
    "    wura_val_urls = wura_ds[\"url\"]\n",
    "    wura_val_urls = {url.strip(\"/\") + \"/\" for url in wura_val_urls}\n",
    "\n",
    "    def format_url(row):\n",
    "        if pd.isna(row.url):\n",
    "            row.url = \"\"\n",
    "            return row\n",
    "        else:\n",
    "            row.url = row.url.strip(\"/\") + \"/\"\n",
    "            return row\n",
    "\n",
    "    df = df.apply(lambda row: format_url(row), axis=1)\n",
    "    df = df[~df.url.isin(wura_val_urls)].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_wura_df(wura_ds):\n",
    "    is_headline_valid = lambda value: len((value or \" \").split()) > 5\n",
    "    is_url_valid = lambda value: len((value or \" \").strip()) > 5\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for row in wura_ds:\n",
    "        if not (is_headline_valid(row[\"headline\"]) and is_url_valid(row[\"url\"])):\n",
    "            continue\n",
    "\n",
    "        data.append({\n",
    "            \"title\": row[\"headline\"], \"sub_topic\": None,\n",
    "            \"url\": row[\"url\"].strip(\"/\") + \"/\", \"text\": row[\"content\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        })\n",
    "\n",
    "    wura_df = pd.DataFrame(data)\n",
    "    return wura_df\n",
    "\n",
    "\n",
    "def align_with_wura(df, wura_data):\n",
    "    df = wura_remove_validation_rows(df, wura_data[\"validation\"])\n",
    "    # Combined collected dataset with Wura train dataset\n",
    "    # wura_df = make_wura_df(wura_data[\"train\"])\n",
    "    wura_df = prepare_wura(wura_data[\"train\"])\n",
    "\n",
    "    df_urls = set(df.url)\n",
    "    seen_rows = wura_df.url.isin(df_urls)\n",
    "    new_wura_df = wura_df[~seen_rows]\n",
    "    old_wura_df = wura_df[seen_rows]\n",
    "    df = pd.concat([df, new_wura_df])\n",
    "    # Extracting the category data available in Wura, so we don't miss out on that data\n",
    "    df[\"category\"] = df[\"url\"].map(old_wura_df.set_index(\"url\")[\"category\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def unify_datasources(dfs: list, wura_data):\n",
    "    for df in dfs:\n",
    "        df.columns = df.columns.str.lower()\n",
    "        if \"sub_topic\" not in df.columns:\n",
    "            df[\"sub_topic\"] = None\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df = align_with_wura(df, wura_data)\n",
    "\n",
    "    # dropna for title and text columns\n",
    "    key_columns = [\"title\", \"text\"]\n",
    "    df.dropna(subset=key_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_yoruba_df():\n",
    "    \"\"\"Combines collected dataset with the wura dataset, ensuring the urls from collected dataset do not appear in wura validation.\"\"\"\n",
    "    wura_data = load_dataset(\"castorini/wura\", \"yor\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "    df1 = pd.read_csv('alaroye_mato_10k.tsv', delimiter=\"\\t\")\n",
    "    df2 = pd.read_csv('von_mato_6k.tsv', delimiter=\"\\t\")\n",
    "    df3 = pd.read_csv('masakhanews_1k.tsv', delimiter=\"\\t\")\n",
    "\n",
    "    df2.rename(columns={'link': 'url'}, inplace=True)\n",
    "    df3.rename(columns={'headline': 'title'}, inplace=True)\n",
    "\n",
    "    df = unify_datasources([df1, df2, df3], wura_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_igbo_df():\n",
    "    \"\"\"Combines collected dataset with the wura dataset, ensuring the urls from collected dataset do not appear in wura validation.\"\"\"\n",
    "    wura_data = load_dataset(\"castorini/wura\", \"ibo\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "    df1 = pd.read_csv(\"igbo_mato_3k.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "    df1.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df = unify_datasources([df1], wura_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_hausa_df():\n",
    "    wura_data = load_dataset(\"castorini/wura\", \"hau\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "    df1 = pd.read_csv(\"hausa_mato_81k.tsv\", delimiter=\"\\t\")\n",
    "    # Key to note that drop duplicates is being done.\n",
    "    # Later on, this should be handled better. DUplicates are being dropped here to avoid potentially\n",
    "    # using the same link as a negative, as at the moment, negatives are being sampled using n-1.\n",
    "    df1 = df1.drop_duplicates([\"link\"])\n",
    "    df1.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df = unify_datasources([df1], wura_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_igbo_df_v0():\n",
    "    df = pd.read_csv(\"igbo_mato_3k.tsv\", delimiter=\"\\t\")\n",
    "    df = df[~(df.title.isna() | df.text.isna())]\n",
    "    df.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df[[\"sub_topic\", \"category\"]] = None\n",
    "    return df\n",
    "\n",
    "def make_hausa_df_v0():\n",
    "    df = pd.read_csv(\"hausa_mato_81k.tsv\", delimiter=\"\\t\")\n",
    "    df = df[~(df.title.isna() | df.text.isna())]\n",
    "    # Key to note that drop duplicates is being done.\n",
    "    # Later on, this should be handled better. DUplicates are being dropped here to avoid potentially\n",
    "    # using the same link as a negative, as at the moment, negatives are being sampled using n-1.\n",
    "    df = df.drop_duplicates([\"link\"])\n",
    "    df.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df[\"category\"] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07922a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_key = \"train\"\n",
    "\n",
    "# domains = {extract_domain_name(row[\"url\"]) for row in data[split_key]}\n",
    "\n",
    "# data_split = data[split_key].add_column(\"domain\", [extract_domain_name(row[\"url\"]) for row in data[split_key]])\n",
    "\n",
    "# # weird_domains = {\"smartkidparenting.com\", \"transferservice-basel.ch\"}\n",
    "\n",
    "# is_valid_value = lambda value: len((value or \" \").strip()) > 5\n",
    "\n",
    "# titled_rows = [row for row in data_split if is_valid_value(row[\"headline\"]) and is_valid_value(row[\"url\"])]\n",
    "\n",
    "# titled_domains = {}\n",
    "\n",
    "# for row in titled_rows:\n",
    "#     url = titled_domains.get(row[\"domain\"], set())\n",
    "#     url.add(row[\"url\"])\n",
    "#     titled_domains[row[\"domain\"]] = url\n",
    "\n",
    "# crawled = {\"yoruba.von.gov.ng\", \"bbc.com\", \"alaroye.org\"}\n",
    "# crawled_complement = set(titled_domains.keys()).difference(crawled)\n",
    "# eval_data = [row for row in data_split if row[\"domain\"] in crawled_complement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12711166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_v2(df, duplicate_rows=False):\n",
    "    \"\"\"In this version of make dataset, we duplicate rows that have title and subtopic, using the title as query in one and subtopic as query in the other.\"\"\"\n",
    "    df_count = len(df)\n",
    "    df[\"neg\"] = None\n",
    "    def pick_negative_values(row):\n",
    "        picked = False\n",
    "        neg = row.neg\n",
    "        if not neg:\n",
    "            size = 7\n",
    "            neg = []\n",
    "        else:\n",
    "            neg = [neg]\n",
    "            size = 6\n",
    "\n",
    "        while not picked:\n",
    "            indexes = np.random.choice(df_count, size=size, replace=False)\n",
    "            if row.name not in indexes:\n",
    "                picked = True\n",
    "\n",
    "        new_neg = neg + df.iloc[indexes].pos.tolist()\n",
    "        return new_neg\n",
    "\n",
    "    df.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "    df[\"neg\"] = df.apply(lambda row: pick_negative_values(row), axis=1)\n",
    "    # Extracting subtopics and using them as a query in duplicate rows\n",
    "    rows_wo_subtopic = df[\"sub_topic\"].isna()\n",
    "    if duplicate_rows:\n",
    "        sub_topic_df = df[~rows_wo_subtopic].copy()\n",
    "        sub_topic_df.loc[:, \"query\"] = sub_topic_df.loc[:, \"sub_topic\"]\n",
    "        df = pd.concat([df, sub_topic_df])\n",
    "    else:\n",
    "        df.loc[~rows_wo_subtopic, \"query\"] = df[~rows_wo_subtopic].sub_topic\n",
    "\n",
    "    # The BGE M3 expects a list of values\n",
    "    df[\"pos\"] = df[\"pos\"].apply(lambda x: [x])\n",
    "    df = df.loc[:, [\"query\", \"pos\", \"neg\"]]\n",
    "    seed = 42\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df.to_json(\"dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    print(df.info())\n",
    "\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.1, random_state=seed, shuffle=True)\n",
    "    train_df.to_json(\"train_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    eval_df.to_json(\"eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def make_dataset_v3(df, duplicate_rows=False, filename=\"train_dataset.jsonl\"):\n",
    "    \"\"\"In this version of make dataset, no longer split into train and eval, because eval and test datasets are currently gotten from wura.\"\"\"\n",
    "    df_count = len(df)\n",
    "    df[\"neg\"] = None\n",
    "    def pick_negative_values(row):\n",
    "        picked = False\n",
    "        neg = row.neg\n",
    "        if not neg:\n",
    "            size = 7\n",
    "            neg = []\n",
    "        else:\n",
    "            neg = [neg]\n",
    "            size = 6\n",
    "\n",
    "        while not picked:\n",
    "            indexes = np.random.choice(df_count, size=size, replace=False)\n",
    "            if row.name not in indexes:\n",
    "                picked = True\n",
    "\n",
    "        new_neg = neg + df.iloc[indexes].pos.tolist()\n",
    "        return new_neg\n",
    "\n",
    "    df.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "    df[\"neg\"] = df.apply(lambda row: pick_negative_values(row), axis=1)\n",
    "    # Extracting subtopics and using them as a query in duplicate rows\n",
    "    rows_wo_subtopic = df[\"sub_topic\"].isna()\n",
    "    if duplicate_rows:\n",
    "        sub_topic_df = df[~rows_wo_subtopic].copy()\n",
    "        sub_topic_df.loc[:, \"query\"] = sub_topic_df.loc[:, \"sub_topic\"]\n",
    "        df = pd.concat([df, sub_topic_df])\n",
    "    else:\n",
    "        df.loc[~rows_wo_subtopic, \"query\"] = df[~rows_wo_subtopic].sub_topic\n",
    "\n",
    "    # The BGE M3 expects a list of values\n",
    "    df[\"pos\"] = df[\"pos\"].apply(lambda x: [x])\n",
    "    df = df.loc[:, [\"query\", \"pos\", \"neg\"]]\n",
    "    seed = 42\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df.to_json(filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ef0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset():\n",
    "    # masakhanews_1k.tsv is from Masakhanews\n",
    "    df1 = pd.read_csv('masakhanews_1k.tsv', delimiter=\"\\t\").drop_duplicates([\"headline\", \"text\"])\n",
    "    df1.dropna(inplace=True)\n",
    "    df1.rename(columns={'headline': 'query', 'text': 'pos'}, inplace=True)\n",
    "    df1.drop(columns=[\"category\", \"url\"], inplace=True)\n",
    "    df1[\"neg\"] = None\n",
    "\n",
    "    # alaroye_mato_10k.tsv is from AbdulMatin's crawl of Alaroye\n",
    "    df2 = pd.read_csv('alaroye_mato_10k.tsv', delimiter=\"\\t\").drop_duplicates([\"Url\"])\n",
    "    df2.dropna(inplace=True)\n",
    "    df2.rename(columns={'Title': 'query', 'Text': 'pos'}, inplace=True)\n",
    "    df2.drop(columns=[\"Url\"], inplace=True)\n",
    "    df2[\"neg\"] = None\n",
    "\n",
    "    # von_mato_6k.tsv is from AbdulMatin's crawl of VON\n",
    "    df3 = pd.read_csv('von_mato_6k.tsv', delimiter=\"\\t\").drop_duplicates([\"link\"])\n",
    "    df3.dropna(inplace=True)\n",
    "    df3.rename(columns={'sub_topic': 'query', 'text': 'pos'}, inplace=True)\n",
    "    df3.drop(columns=[\"title\", \"link\"], inplace=True)\n",
    "    df3[\"neg\"] = None\n",
    "\n",
    "    df = pd.concat([df1, df2, df3])\n",
    "\n",
    "    df_count = len(df)\n",
    "    def pick_negative_values(row):\n",
    "        picked = False\n",
    "        neg = row.neg\n",
    "        if not neg:\n",
    "            size = 7\n",
    "            neg = []\n",
    "        else:\n",
    "            neg = [neg]\n",
    "            size = 6\n",
    "\n",
    "        while not picked:\n",
    "            indexes = np.random.choice(df_count, size=size, replace=False)\n",
    "            if row.name not in indexes:\n",
    "                picked = True\n",
    "\n",
    "        new_neg = neg + df.iloc[indexes].pos.tolist()\n",
    "        return new_neg\n",
    "\n",
    "    # Apply function to each row\n",
    "    seed = 42\n",
    "    df[\"neg\"] = df.apply(lambda row: pick_negative_values(row), axis=1)\n",
    "    df[\"pos\"] = df[\"pos\"].apply(lambda x: [x])\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df.to_json(\"dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    print(df.info())\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    train_df.to_json(\"train_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    test_df, eval_df = train_test_split(test_df, test_size=0.5, random_state=seed, shuffle=True)\n",
    "    eval_df.to_json(\"eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    test_df.to_json(\"test_dataset.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20448cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def combine_wura_with_all_mato_igbo():\n",
    "    \"\"\"Just adding igbo data to yoruba's train data, to see if it improves quality of yoruba data.\"\"\"\n",
    "    # Creates train_dataset.jsonl, dataset.jsonl and eval_dataset.jsonl. But dataset.jsonl is the important one.\n",
    "    make_dataset_v2(make_igbo_df_v0())\n",
    "    # Overwrite the train_dataset.jsonl\n",
    "    combine_wura_train = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/combined_wura/train_dataset.jsonl\"\n",
    "    shutil.copyfile(combine_wura_train, \"train_dataset.jsonl\")\n",
    "\n",
    "    data = []\n",
    "    with jsonlines.open(\"train_dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    with jsonlines.open(\"dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    with jsonlines.open(\"train_dataset.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "def combine_wura_with_all_mato_igbo_hausa():\n",
    "    \"\"\"Just adding igbo+hausa data to yoruba's train data, to see if it improves quality of yoruba data.\"\"\"\n",
    "    # Creates train_dataset.jsonl, dataset.jsonl and eval_dataset.jsonl. But dataset.jsonl is the important one.\n",
    "    df = pd.concat([make_igbo_df_v0(), make_hausa_df_v0()])\n",
    "    make_dataset_v2(df)\n",
    "    # Overwrite the train_dataset.jcomsonl\n",
    "    combine_wura_train = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/combined_wura/train_dataset.jsonl\"\n",
    "    shutil.copyfile(combine_wura_train, \"train_dataset.jsonl\")\n",
    "\n",
    "    data = []\n",
    "    with jsonlines.open(\"train_dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    with jsonlines.open(\"dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    with jsonlines.open(\"train_dataset.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "    hausa_igbo_comwura_train = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/hausa_igbo_comwura/train_dataset.jsonl\"\n",
    "    shutil.copyfile(\"train_dataset.jsonl\", hausa_igbo_comwura_train)\n",
    "\n",
    "\n",
    "def make_incremental_igbo_hausa_eval_datasets():\n",
    "    \"\"\"Samples from the hausa and igbo wura train datasets\"\"\"\n",
    "    def make_incremental(lang_id):\n",
    "        data = load_dataset(\"castorini/wura\", lang_id, level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "        dataset = make_wura_df(data[\"train\"])\n",
    "\n",
    "        random.seed(SEED)\n",
    "        eval_idxs = random.sample(range(len(dataset)), 2000)\n",
    "        eval_dataset = dataset.iloc[eval_idxs]\n",
    "\n",
    "        eval_dataset.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "        eval_dataset.to_json(f\"{lang_id}_eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    make_incremental(\"ibo\")\n",
    "    make_incremental(\"hau\")\n",
    "\n",
    "    eval_dataset = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/igbo/eval_dataset.jsonl\"\n",
    "    shutil.copyfile(\"ibo_eval_dataset.jsonl\", eval_dataset)\n",
    "\n",
    "    eval_dataset = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/hausa/eval_dataset.jsonl\"\n",
    "    shutil.copyfile(\"hau_eval_dataset.jsonl\", eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb4d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "def text_to_guid(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic GUID (UUID v5) from a given text.\n",
    "    \"\"\"\n",
    "    namespace = uuid.NAMESPACE_DNS  # Standard namespace, or use your own UUID\n",
    "    return str(uuid.uuid5(namespace, text))\n",
    "\n",
    "\n",
    "def format_evaluation_jsonl(filepath):\n",
    "    filepath = Path(filepath)\n",
    "    lines = []\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        for obj in reader:\n",
    "            lines.append(obj)\n",
    "\n",
    "\n",
    "    dataset = {\"queries\": {}, \"corpus\": {}, \"relevant_docs\": {}, \"mode\": \"text\"}\n",
    "\n",
    "    for line in lines:\n",
    "        query_id = text_to_guid(line[\"query\"])\n",
    "        if isinstance(line[\"pos\"], str):\n",
    "            pos = line[\"pos\"]\n",
    "        elif isinstance(line[\"pos\"], list):\n",
    "            pos = line[\"pos\"][0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type for 'pos': {type(line['pos'])}. Expected a list or string.\")\n",
    "\n",
    "        pos_id = text_to_guid(pos)\n",
    "        dataset[\"queries\"][query_id] = line[\"query\"]\n",
    "        dataset[\"corpus\"][pos_id] = pos\n",
    "        dataset[\"relevant_docs\"][query_id] = [pos_id]\n",
    "\n",
    "    new_path = filepath.parent / (filepath.stem + \"_formatted.json\")\n",
    "    with open(new_path, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92e07665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = split_wura_validation_all_langs()\n",
    "\n",
    "# !cp hau_eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/hausa/eval_dataset.jsonl\n",
    "# !cp hau_test_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/hausa/test_dataset.jsonl\n",
    "\n",
    "# !cp igbo_eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/igbo/eval_dataset.jsonl\n",
    "# !cp igbo_test_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/igbo/test_dataset.jsonl\n",
    "\n",
    "# !cp yor_eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/yoruba/eval_dataset.jsonl\n",
    "# !cp yor_test_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/yoruba/test_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b669c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 359881/359881 [00:19<00:00, 18891.78 examples/s]\n",
      "Generating validation split: 100%|██████████| 39986/39986 [00:02<00:00, 19208.35 examples/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hausa_mato_81k.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# make_dataset_v3(make_yoruba_df(), filename=\"yor_train_dataset.jsonl\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# make_dataset_v3(make_igbo_df(), filename=\"igbo_train_dataset.jsonl\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m make_dataset_v3(\u001b[43mmake_hausa_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhausa_train_dataset.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# format_evaluation_jsonl(\"eval_dataset.jsonl\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# OR\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Download dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# !gdown https://drive.google.com/uc?id=1xJ6EHSyaZeMtosQ7RF_R9OHJssuXl0Eq\u001b[39;00m\n\u001b[1;32m     12\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgdown https://drive.google.com/uc?id=1qR1n_kb5mtCfbAPitw3bffRKQtN0H-ZL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 97\u001b[0m, in \u001b[0;36mmake_hausa_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_hausa_df\u001b[39m():\n\u001b[1;32m     96\u001b[0m     wura_data \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcastorini/wura\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhau\u001b[39m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, verification_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_checks\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 97\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhausa_mato_81k.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Key to note that drop duplicates is being done.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Later on, this should be handled better. DUplicates are being dropped here to avoid potentially\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# using the same link as a negative, as at the moment, negatives are being sampled using n-1.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mdrop_duplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hausa_mato_81k.tsv'"
     ]
    }
   ],
   "source": [
    "# make_dataset_v3(make_yoruba_df(), filename=\"yor_train_dataset.jsonl\")\n",
    "# make_dataset_v3(make_igbo_df(), filename=\"igbo_train_dataset.jsonl\")\n",
    "make_dataset_v3(make_hausa_df(), filename=\"hausa_train_dataset.jsonl\")\n",
    "# format_evaluation_jsonl(\"eval_dataset.jsonl\")\n",
    "\n",
    "# OR\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "# !gdown https://drive.google.com/uc?id=1xJ6EHSyaZeMtosQ7RF_R9OHJssuXl0Eq\n",
    "\n",
    "!gdown https://drive.google.com/uc?id=1qR1n_kb5mtCfbAPitw3bffRKQtN0H-ZL\n",
    "\n",
    "\n",
    "!gdown https://drive.google.com/uc?id=10RHg1qWjopgjo0Ns0TZ53zhhAmVuO6u4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bffe08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1M1YTH2jYJ6zL8T_k4Icxe7RkwlBcBB9d\n",
      "From (redirected): https://drive.google.com/uc?id=1M1YTH2jYJ6zL8T_k4Icxe7RkwlBcBB9d&confirm=t&uuid=26d7e3f1-a605-4688-acea-03c9a7cc58d2\n",
      "To: /home/omotoso.abdulmatin4/filtered_yoruba_train_dataset.jsonl\n",
      "100%|█████████████████████████████████████████| 307M/307M [00:01<00:00, 187MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1M1YTH2jYJ6zL8T_k4Icxe7RkwlBcBB9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/hausa_train_dataset.jsonl .\n",
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/igbo_train_dataset.jsonl .\n",
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/yoruba_train_dataset.jsonl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with jsonlines.open(\"yoruba_train_dataset.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)\n",
    "\n",
    "with jsonlines.open(\"hausa_train_dataset.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)\n",
    "\n",
    "with jsonlines.open(\"igbo_train_dataset.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [10_000, 50_000, 100_000, 300_000]\n",
    "\n",
    "for size in sizes:\n",
    "    with jsonlines.open(f\"{size}_train_dataset.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(data[:size])\n",
    "\n",
    "# 10k\n",
    "gdown https://drive.google.com/uc?id=1qR1n_kb5mtCfbAPitw3bffRKQtN0H-ZL\n",
    "# 50k\n",
    "gdown https://drive.google.com/uc?id=1-2UiPWc6Z0Qn0coN1yYIgOSciNFcEncB\n",
    "# 100k\n",
    "gdown https://drive.google.com/uc?id=1-4WNTv69iQR528_lS7iKQxo24jkBnECr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp 10000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/10000_train_dataset.jsonl\n",
    "!cp 50000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/50000_train_dataset.jsonl\n",
    "!cp 100000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/100000_train_dataset.jsonl\n",
    "!cp 300000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/300000_train_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/combine_wura_all_langs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6036dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/hausa_mato_81k.tsv .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/igbo_mato_3k.tsv .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/alaroye_mato_10k.tsv .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/von_mato_6k.tsv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07694b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets\n",
    "# !cp eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets\n",
    "# !cp train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/igbo_comwura/\n",
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/hausa/eval_dataset.jsonl .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/hausa_igbo_comwura/train_dataset.jsonl .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combined_wura/eval_dataset.jsonl .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/test_dataset.jsonl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_evaluation_jsonl(\"eval_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60a5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-18 16:53:03--  https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/refs/heads/master/examples/finetune/ds_stage0.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 963 [text/plain]\n",
      "Saving to: ‘ds_stage0.json’\n",
      "\n",
      "ds_stage0.json      100%[===================>]     963  --.-KB/s    in 0s      \n",
      "\n",
      "2025-04-18 16:53:03 (105 MB/s) - ‘ds_stage0.json’ saved [963/963]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/refs/heads/master/examples/finetune/ds_stage0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73470bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1272319703.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    torchrun --standalone --nproc_per_node 1 \\\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "torchrun --standalone --nproc_per_node 1 \\\n",
    "-m FlagEmbedding.finetune.embedder.encoder_only.m3 \\\n",
    "--model_name_or_path BAAI/bge-m3 \\\n",
    "--output_dir ./bge-m3 \\\n",
    "--cache_dir ./cache/model \\\n",
    "--cache_path ./cache/data \\\n",
    "--train_data ./filtered_yoruba_train_dataset.jsonl_train_dataset.jsonl \\\n",
    "--trust_remote_code True \\\n",
    "--train_group_size 2 \\\n",
    "--query_max_len 512 \\\n",
    "--passage_max_len 2048 \\\n",
    "--overwrite_output_dir \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--dataloader_num_workers 12 \\\n",
    "--gradient_checkpointing \\\n",
    "--deepspeed ds_stage0.json \\\n",
    "--num_train_epochs 3 \\\n",
    "--per_device_train_batch_size 160 \\\n",
    "--dataloader_drop_last False \\\n",
    "--warmup_ratio 0.1 \\\n",
    "--report_to none \\\n",
    "--logging_steps 100 \\\n",
    "--save_steps 500 \\\n",
    "--temperature 0.01 \\\n",
    "--sentence_pooling_method cls \\\n",
    "--normalize_embeddings True \\\n",
    "--knowledge_distillation False \\\n",
    "--kd_loss_type m3_kd_loss \\\n",
    "--unified_finetuning False \\\n",
    "--use_self_distill False \\\n",
    "--fix_encoder False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fb8558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torchrun --standalone --nproc_per_node 4 -m FlagEmbedding.finetune.embedder.encoder_only.m3 --model_name_or_path BAAI/bge-m3 --output_dir ./bge-m3 --cache_dir ./cache/model --cache_path ./cache/data --train_data /home/omotoso.abdulmatin4/filtered_yoruba_train_dataset.jsonl --trust_remote_code True --train_group_size 2 --query_max_len 512 --passage_max_len 2048 --overwrite_output_dir --learning_rate 1e-5 --fp16 --dataloader_num_workers 12 --gradient_checkpointing --deepspeed ds_stage0.json --num_train_epochs 3 --per_device_train_batch_size 8 --dataloader_drop_last False --warmup_ratio 0.1 --report_to none --logging_steps 100 --save_steps 500 --temperature 0.01 --sentence_pooling_method cls --normalize_embeddings True --knowledge_distillation False --kd_loss_type m3_kd_loss --unified_finetuning False --use_self_distill False --fix_encoder False\n"
     ]
    }
   ],
   "source": [
    "# # Train a model, terminal command\n",
    "import re\n",
    "\n",
    "command = \"\"\"\n",
    "torchrun --standalone --nproc_per_node 4 \\\n",
    "-m FlagEmbedding.finetune.embedder.encoder_only.m3 \\\n",
    "--model_name_or_path BAAI/bge-m3 \\\n",
    "--output_dir ./bge-m3 \\\n",
    "--cache_dir ./cache/model \\\n",
    "--cache_path ./cache/data \\\n",
    "--train_data /home/omotoso.abdulmatin4/filtered_yoruba_train_dataset.jsonl \\\n",
    "--trust_remote_code True \\\n",
    "--train_group_size 2 \\\n",
    "--query_max_len 512 \\\n",
    "--passage_max_len 2048 \\\n",
    "--overwrite_output_dir \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--dataloader_num_workers 12 \\\n",
    "--gradient_checkpointing \\\n",
    "--deepspeed ds_stage0.json \\\n",
    "--num_train_epochs 3 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--dataloader_drop_last False \\\n",
    "--warmup_ratio 0.1 \\\n",
    "--report_to none \\\n",
    "--logging_steps 100 \\\n",
    "--save_steps 500 \\\n",
    "--temperature 0.01 \\\n",
    "--sentence_pooling_method cls \\\n",
    "--normalize_embeddings True \\\n",
    "--knowledge_distillation False \\\n",
    "--kd_loss_type m3_kd_loss \\\n",
    "--unified_finetuning False \\\n",
    "--use_self_distill False \\\n",
    "--fix_encoder False\"\"\"\n",
    "\n",
    "command = re.sub(r'\\\\\\n\\s+', '', command)\n",
    "\n",
    "print(command)\n",
    "\n",
    "# OR\n",
    "\n",
    "# Download existing model weights\n",
    "# !gdown https://drive.google.com/uc?id=1hC2nReprpHpCNWq9yergzGJLSHz_VKia\n",
    "# !tar -xzvf bge-m3-5-epochs-unified.tar.gz\n",
    "\n",
    "#gdown https://drive.google.com/uc?id=1-2UiPWc6Z0Qn0coN1yYIgOSciNFcEncB\n",
    "#gdown https://drive.google.com/uc?id=1-4WNTv69iQR528_lS7iKQxo24jkBnECr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b1866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-18 23:27:56,300] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "[2025-04-18 23:27:56,300] torch.distributed.run: [WARNING] \n",
      "[2025-04-18 23:27:56,300] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-04-18 23:27:56,300] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2025-04-18 23:27:56,300] torch.distributed.run: [WARNING] *****************************************\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940137ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}\n",
    "\n",
    "model_id = \"bge-m3-hausaigbocomwura-2_95-epochs-e5-lr-0_1-warmup-32-batchsize-0_01-temperature-2-groupsize\"\n",
    "!tar --exclude='global_*' -czvf {model_id}.tar.gz ./bge-m3/checkpoint-12500\n",
    "!cp {model_id}.tar.gz /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/experiments/model_weights/\n",
    "\n",
    "model_id = \"bge-m3-hausaigbocomwura-3-epochs-e5-lr-0_1-warmup-32-batchsize-0_01-temperature-2-groupsize\"\n",
    "!tar --exclude='./bge-m3/checkpoint-*' -czvf {model_id}.tar.gz ./bge-m3\n",
    "!cp {model_id}.tar.gz /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/experiments/model_weights/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv-env)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
