{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa292f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c038f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, arrow_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import jsonlines\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c071e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset(\"castorini/wura\", \"yor\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "data = load_dataset(\"castorini/wura\", \"ibo\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73660d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wura(dataset):\n",
    "    if not isinstance(dataset, arrow_dataset.Dataset):\n",
    "        raise ValueError(f\"The parameter `dataset` only accepts `arrow_dataset.Dataset` objects. Got {type(dataset)} instead.\")\n",
    "\n",
    "    expected_columns = {\"headline\", \"content\", \"category\", \"url\"}\n",
    "    missing_columns = expected_columns.difference(set(dataset.features))\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The dataset must contain all of the following features: {expected_columns}. Missing features: {missing_columns}\")\n",
    "\n",
    "    domain_counts = {}\n",
    "    for row in dataset:\n",
    "        domain = extract_domain_name(row[\"url\"])\n",
    "        domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "    invalid_domains = {\n",
    "        \"jw.org\" # Has really weird links, for example:  https://www.jw.org/yo/elerii-jehofa/kan-si-wa/venezuela/, https://www.jw.org/yo/elerii-jehofa/kan-si-wa/tonga/, https://www.jw.org/yo/elerii-jehofa/kan-si-wa/taiwan/ all have the title \"Kan Si Wa\"\n",
    "    }\n",
    "\n",
    "    is_headline_valid = lambda value: len((value or \" \").split()) > 1\n",
    "    is_url_valid = lambda value: len((value or \" \").strip()) > 5\n",
    "    is_domain_valid = lambda value: domain_counts[value] > 10 and not value in invalid_domains # If the domain does not appear enough times that is a sign that the site is not committed to publishing in the language. So it is probably a weird url or the English was translated using Google translate e.g. https://downloadfacetime.com/facetime/facetime-for-ipad/\n",
    "    is_text_valid = lambda value: len((value or \" \").strip().split()) > 30\n",
    "\n",
    "    data = []\n",
    "    for row in dataset:\n",
    "        if not (is_headline_valid(row[\"headline\"]) \\\n",
    "                and is_url_valid(row[\"url\"]) \\\n",
    "                and is_domain_valid(extract_domain_name(row[\"url\"]))):\n",
    "            continue\n",
    "\n",
    "        data.append({\n",
    "            \"title\": row[\"headline\"],\n",
    "            \"url\": row[\"url\"].strip(\"/\") + \"/\", \"text\": row[\"content\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        })\n",
    "\n",
    "    wura_df = pd.DataFrame(data)\n",
    "    return wura_df\n",
    "\n",
    "\n",
    "def split_wura_validation_all_langs():\n",
    "    languages = [\"yor\", \"igbo\", \"hau\"]\n",
    "    dfs = {}\n",
    "    for lang in languages:\n",
    "        wura_lang = \"ibo\" if lang == \"igbo\" else lang\n",
    "        dataset = load_dataset(\"castorini/wura\", wura_lang, level=\"document\", trust_remote_code=True)\n",
    "        validation_data = dataset.get(\"validation\")\n",
    "        if not validation_data:\n",
    "            raise ValueError(f\"Dataset {wura_lang} does not have a validation split. Only found {dataset.keys()} splits.\")\n",
    "        lang_df = prepare_wura(validation_data)\n",
    "        lang_df.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "        eval_df, test_df = train_test_split(lang_df, test_size=0.4, random_state=SEED, shuffle=True)\n",
    "        eval_df.to_json(f\"{lang}_eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "        test_df.to_json(f\"{lang}_test_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "        dfs[lang] = {\n",
    "            \"eval\": eval_df,\n",
    "            \"test\": test_df\n",
    "        }\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548cb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"bbc.com\"\n",
    "# df[df[\"domain_name\"] == domain].url.tolist()\n",
    "# df[df[\"domain_name\"] == domain].head(15).pos.tolist()\n",
    "# df[df[\"domain_name\"] == domain].head(15).url.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed474bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def extract_domain_name(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        netloc = str(parsed_url.netloc)\n",
    "        return netloc.strip(\"www.\")\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c59a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wura_remove_validation_rows(df, wura_ds):\n",
    "    \"\"\"Checks for rows in df that exist in wura_ds, using the url, then drops them\"\"\"\n",
    "    wura_val_urls = wura_ds[\"url\"]\n",
    "    wura_val_urls = {url.strip(\"/\") + \"/\" for url in wura_val_urls}\n",
    "\n",
    "    def format_url(row):\n",
    "        if pd.isna(row.url):\n",
    "            row.url = \"\"\n",
    "            return row\n",
    "        else:\n",
    "            row.url = row.url.strip(\"/\") + \"/\"\n",
    "            return row\n",
    "\n",
    "    df = df.apply(lambda row: format_url(row), axis=1)\n",
    "    df = df[~df.url.isin(wura_val_urls)].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_wura_df(wura_ds):\n",
    "    is_headline_valid = lambda value: len((value or \" \").split()) > 5\n",
    "    is_url_valid = lambda value: len((value or \" \").strip()) > 5\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for row in wura_ds:\n",
    "        if not (is_headline_valid(row[\"headline\"]) and is_url_valid(row[\"url\"])):\n",
    "            continue\n",
    "\n",
    "        data.append({\n",
    "            \"title\": row[\"headline\"], \"sub_topic\": None,\n",
    "            \"url\": row[\"url\"].strip(\"/\") + \"/\", \"text\": row[\"content\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        })\n",
    "\n",
    "    wura_df = pd.DataFrame(data)\n",
    "    return wura_df\n",
    "\n",
    "\n",
    "def align_with_wura(df, wura_data):\n",
    "    df = wura_remove_validation_rows(df, wura_data[\"validation\"])\n",
    "    # Combined collected dataset with Wura train dataset\n",
    "    # wura_df = make_wura_df(wura_data[\"train\"])\n",
    "    wura_df = prepare_wura(wura_data[\"train\"])\n",
    "\n",
    "    df_urls = set(df.url)\n",
    "    seen_rows = wura_df.url.isin(df_urls)\n",
    "    new_wura_df = wura_df[~seen_rows]\n",
    "    old_wura_df = wura_df[seen_rows]\n",
    "    df = pd.concat([df, new_wura_df])\n",
    "    # Extracting the category data available in Wura, so we don't miss out on that data\n",
    "    df[\"category\"] = df[\"url\"].map(old_wura_df.set_index(\"url\")[\"category\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def unify_datasources(dfs: list, wura_data):\n",
    "    for df in dfs:\n",
    "        df.columns = df.columns.str.lower()\n",
    "        if \"sub_topic\" not in df.columns:\n",
    "            df[\"sub_topic\"] = None\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df = align_with_wura(df, wura_data)\n",
    "\n",
    "    # dropna for title and text columns\n",
    "    key_columns = [\"title\", \"text\"]\n",
    "    df.dropna(subset=key_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_yoruba_df():\n",
    "    \"\"\"Combines collected dataset with the wura dataset, ensuring the urls from collected dataset do not appear in wura validation.\"\"\"\n",
    "    wura_data = load_dataset(\"castorini/wura\", \"yor\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "    df1 = pd.read_csv('alaroye_mato_10k.tsv', delimiter=\"\\t\")\n",
    "    df2 = pd.read_csv('von_mato_6k.tsv', delimiter=\"\\t\")\n",
    "    df3 = pd.read_csv('masakhanews_1k.tsv', delimiter=\"\\t\")\n",
    "\n",
    "    df2.rename(columns={'link': 'url'}, inplace=True)\n",
    "    df3.rename(columns={'headline': 'title'}, inplace=True)\n",
    "\n",
    "    df = unify_datasources([df1, df2, df3], wura_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_igbo_df():\n",
    "    \"\"\"Combines collected dataset with the wura dataset, ensuring the urls from collected dataset do not appear in wura validation.\"\"\"\n",
    "    wura_data = load_dataset(\"castorini/wura\", \"ibo\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "    df1 = pd.read_csv(\"igbo_mato_3k.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "    df1.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df = unify_datasources([df1], wura_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_hausa_df():\n",
    "    wura_data = load_dataset(\"castorini/wura\", \"hau\", level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "    df1 = pd.read_csv(\"hausa_mato_81k.tsv\", delimiter=\"\\t\")\n",
    "    # Key to note that drop duplicates is being done.\n",
    "    # Later on, this should be handled better. DUplicates are being dropped here to avoid potentially\n",
    "    # using the same link as a negative, as at the moment, negatives are being sampled using n-1.\n",
    "    df1 = df1.drop_duplicates([\"link\"])\n",
    "    df1.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df = unify_datasources([df1], wura_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_igbo_df_v0():\n",
    "    df = pd.read_csv(\"igbo_mato_3k.tsv\", delimiter=\"\\t\")\n",
    "    df = df[~(df.title.isna() | df.text.isna())]\n",
    "    df.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df[[\"sub_topic\", \"category\"]] = None\n",
    "    return df\n",
    "\n",
    "def make_hausa_df_v0():\n",
    "    df = pd.read_csv(\"hausa_mato_81k.tsv\", delimiter=\"\\t\")\n",
    "    df = df[~(df.title.isna() | df.text.isna())]\n",
    "    # Key to note that drop duplicates is being done.\n",
    "    # Later on, this should be handled better. DUplicates are being dropped here to avoid potentially\n",
    "    # using the same link as a negative, as at the moment, negatives are being sampled using n-1.\n",
    "    df = df.drop_duplicates([\"link\"])\n",
    "    df.rename(columns={\"link\": \"url\"}, inplace=True)\n",
    "    df[\"category\"] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07922a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_key = \"train\"\n",
    "\n",
    "# domains = {extract_domain_name(row[\"url\"]) for row in data[split_key]}\n",
    "\n",
    "# data_split = data[split_key].add_column(\"domain\", [extract_domain_name(row[\"url\"]) for row in data[split_key]])\n",
    "\n",
    "# # weird_domains = {\"smartkidparenting.com\", \"transferservice-basel.ch\"}\n",
    "\n",
    "# is_valid_value = lambda value: len((value or \" \").strip()) > 5\n",
    "\n",
    "# titled_rows = [row for row in data_split if is_valid_value(row[\"headline\"]) and is_valid_value(row[\"url\"])]\n",
    "\n",
    "# titled_domains = {}\n",
    "\n",
    "# for row in titled_rows:\n",
    "#     url = titled_domains.get(row[\"domain\"], set())\n",
    "#     url.add(row[\"url\"])\n",
    "#     titled_domains[row[\"domain\"]] = url\n",
    "\n",
    "# crawled = {\"yoruba.von.gov.ng\", \"bbc.com\", \"alaroye.org\"}\n",
    "# crawled_complement = set(titled_domains.keys()).difference(crawled)\n",
    "# eval_data = [row for row in data_split if row[\"domain\"] in crawled_complement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12711166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_v2(df, duplicate_rows=False):\n",
    "    \"\"\"In this version of make dataset, we duplicate rows that have title and subtopic, using the title as query in one and subtopic as query in the other.\"\"\"\n",
    "    df_count = len(df)\n",
    "    df[\"neg\"] = None\n",
    "    def pick_negative_values(row):\n",
    "        picked = False\n",
    "        neg = row.neg\n",
    "        if not neg:\n",
    "            size = 7\n",
    "            neg = []\n",
    "        else:\n",
    "            neg = [neg]\n",
    "            size = 6\n",
    "\n",
    "        while not picked:\n",
    "            indexes = np.random.choice(df_count, size=size, replace=False)\n",
    "            if row.name not in indexes:\n",
    "                picked = True\n",
    "\n",
    "        new_neg = neg + df.iloc[indexes].pos.tolist()\n",
    "        return new_neg\n",
    "\n",
    "    df.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "    df[\"neg\"] = df.apply(lambda row: pick_negative_values(row), axis=1)\n",
    "    # Extracting subtopics and using them as a query in duplicate rows\n",
    "    rows_wo_subtopic = df[\"sub_topic\"].isna()\n",
    "    if duplicate_rows:\n",
    "        sub_topic_df = df[~rows_wo_subtopic].copy()\n",
    "        sub_topic_df.loc[:, \"query\"] = sub_topic_df.loc[:, \"sub_topic\"]\n",
    "        df = pd.concat([df, sub_topic_df])\n",
    "    else:\n",
    "        df.loc[~rows_wo_subtopic, \"query\"] = df[~rows_wo_subtopic].sub_topic\n",
    "\n",
    "    # The BGE M3 expects a list of values\n",
    "    df[\"pos\"] = df[\"pos\"].apply(lambda x: [x])\n",
    "    df = df.loc[:, [\"query\", \"pos\", \"neg\"]]\n",
    "    seed = 42\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df.to_json(\"dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    print(df.info())\n",
    "\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.1, random_state=seed, shuffle=True)\n",
    "    train_df.to_json(\"train_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    eval_df.to_json(\"eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def make_dataset_v3(df, duplicate_rows=False, filename=\"train_dataset.jsonl\"):\n",
    "    \"\"\"In this version of make dataset, no longer split into train and eval, because eval and test datasets are currently gotten from wura.\"\"\"\n",
    "    df_count = len(df)\n",
    "    df[\"neg\"] = None\n",
    "    def pick_negative_values(row):\n",
    "        picked = False\n",
    "        neg = row.neg\n",
    "        if not neg:\n",
    "            size = 7\n",
    "            neg = []\n",
    "        else:\n",
    "            neg = [neg]\n",
    "            size = 6\n",
    "\n",
    "        while not picked:\n",
    "            indexes = np.random.choice(df_count, size=size, replace=False)\n",
    "            if row.name not in indexes:\n",
    "                picked = True\n",
    "\n",
    "        new_neg = neg + df.iloc[indexes].pos.tolist()\n",
    "        return new_neg\n",
    "\n",
    "    df.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "    df[\"neg\"] = df.apply(lambda row: pick_negative_values(row), axis=1)\n",
    "    # Extracting subtopics and using them as a query in duplicate rows\n",
    "    rows_wo_subtopic = df[\"sub_topic\"].isna()\n",
    "    if duplicate_rows:\n",
    "        sub_topic_df = df[~rows_wo_subtopic].copy()\n",
    "        sub_topic_df.loc[:, \"query\"] = sub_topic_df.loc[:, \"sub_topic\"]\n",
    "        df = pd.concat([df, sub_topic_df])\n",
    "    else:\n",
    "        df.loc[~rows_wo_subtopic, \"query\"] = df[~rows_wo_subtopic].sub_topic\n",
    "\n",
    "    # The BGE M3 expects a list of values\n",
    "    df[\"pos\"] = df[\"pos\"].apply(lambda x: [x])\n",
    "    df = df.loc[:, [\"query\", \"pos\", \"neg\"]]\n",
    "    seed = 42\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df.to_json(filename, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ef0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset():\n",
    "    # masakhanews_1k.tsv is from Masakhanews\n",
    "    df1 = pd.read_csv('masakhanews_1k.tsv', delimiter=\"\\t\").drop_duplicates([\"headline\", \"text\"])\n",
    "    df1.dropna(inplace=True)\n",
    "    df1.rename(columns={'headline': 'query', 'text': 'pos'}, inplace=True)\n",
    "    df1.drop(columns=[\"category\", \"url\"], inplace=True)\n",
    "    df1[\"neg\"] = None\n",
    "\n",
    "    # alaroye_mato_10k.tsv is from AbdulMatin's crawl of Alaroye\n",
    "    df2 = pd.read_csv('alaroye_mato_10k.tsv', delimiter=\"\\t\").drop_duplicates([\"Url\"])\n",
    "    df2.dropna(inplace=True)\n",
    "    df2.rename(columns={'Title': 'query', 'Text': 'pos'}, inplace=True)\n",
    "    df2.drop(columns=[\"Url\"], inplace=True)\n",
    "    df2[\"neg\"] = None\n",
    "\n",
    "    # von_mato_6k.tsv is from AbdulMatin's crawl of VON\n",
    "    df3 = pd.read_csv('von_mato_6k.tsv', delimiter=\"\\t\").drop_duplicates([\"link\"])\n",
    "    df3.dropna(inplace=True)\n",
    "    df3.rename(columns={'sub_topic': 'query', 'text': 'pos'}, inplace=True)\n",
    "    df3.drop(columns=[\"title\", \"link\"], inplace=True)\n",
    "    df3[\"neg\"] = None\n",
    "\n",
    "    df = pd.concat([df1, df2, df3])\n",
    "\n",
    "    df_count = len(df)\n",
    "    def pick_negative_values(row):\n",
    "        picked = False\n",
    "        neg = row.neg\n",
    "        if not neg:\n",
    "            size = 7\n",
    "            neg = []\n",
    "        else:\n",
    "            neg = [neg]\n",
    "            size = 6\n",
    "\n",
    "        while not picked:\n",
    "            indexes = np.random.choice(df_count, size=size, replace=False)\n",
    "            if row.name not in indexes:\n",
    "                picked = True\n",
    "\n",
    "        new_neg = neg + df.iloc[indexes].pos.tolist()\n",
    "        return new_neg\n",
    "\n",
    "    # Apply function to each row\n",
    "    seed = 42\n",
    "    df[\"neg\"] = df.apply(lambda row: pick_negative_values(row), axis=1)\n",
    "    df[\"pos\"] = df[\"pos\"].apply(lambda x: [x])\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    df.to_json(\"dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    print(df.info())\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    train_df.to_json(\"train_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    test_df, eval_df = train_test_split(test_df, test_size=0.5, random_state=seed, shuffle=True)\n",
    "    eval_df.to_json(\"eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "    test_df.to_json(\"test_dataset.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20448cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def combine_wura_with_all_mato_igbo():\n",
    "    \"\"\"Just adding igbo data to yoruba's train data, to see if it improves quality of yoruba data.\"\"\"\n",
    "    # Creates train_dataset.jsonl, dataset.jsonl and eval_dataset.jsonl. But dataset.jsonl is the important one.\n",
    "    make_dataset_v2(make_igbo_df_v0())\n",
    "    # Overwrite the train_dataset.jsonl\n",
    "    combine_wura_train = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/combined_wura/train_dataset.jsonl\"\n",
    "    shutil.copyfile(combine_wura_train, \"train_dataset.jsonl\")\n",
    "\n",
    "    data = []\n",
    "    with jsonlines.open(\"train_dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    with jsonlines.open(\"dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    with jsonlines.open(\"train_dataset.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "def combine_wura_with_all_mato_igbo_hausa():\n",
    "    \"\"\"Just adding igbo+hausa data to yoruba's train data, to see if it improves quality of yoruba data.\"\"\"\n",
    "    # Creates train_dataset.jsonl, dataset.jsonl and eval_dataset.jsonl. But dataset.jsonl is the important one.\n",
    "    df = pd.concat([make_igbo_df_v0(), make_hausa_df_v0()])\n",
    "    make_dataset_v2(df)\n",
    "    # Overwrite the train_dataset.jcomsonl\n",
    "    combine_wura_train = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/combined_wura/train_dataset.jsonl\"\n",
    "    shutil.copyfile(combine_wura_train, \"train_dataset.jsonl\")\n",
    "\n",
    "    data = []\n",
    "    with jsonlines.open(\"train_dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    with jsonlines.open(\"dataset.jsonl\") as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    with jsonlines.open(\"train_dataset.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "    hausa_igbo_comwura_train = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/hausa_igbo_comwura/train_dataset.jsonl\"\n",
    "    shutil.copyfile(\"train_dataset.jsonl\", hausa_igbo_comwura_train)\n",
    "\n",
    "\n",
    "def make_incremental_igbo_hausa_eval_datasets():\n",
    "    \"\"\"Samples from the hausa and igbo wura train datasets\"\"\"\n",
    "    def make_incremental(lang_id):\n",
    "        data = load_dataset(\"castorini/wura\", lang_id, level=\"document\", verification_mode=\"no_checks\", trust_remote_code=True)\n",
    "        dataset = make_wura_df(data[\"train\"])\n",
    "\n",
    "        random.seed(SEED)\n",
    "        eval_idxs = random.sample(range(len(dataset)), 2000)\n",
    "        eval_dataset = dataset.iloc[eval_idxs]\n",
    "\n",
    "        eval_dataset.rename(columns={\"text\": \"pos\", \"title\": \"query\"}, inplace=True)\n",
    "        eval_dataset.to_json(f\"{lang_id}_eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "    make_incremental(\"ibo\")\n",
    "    make_incremental(\"hau\")\n",
    "\n",
    "    eval_dataset = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/igbo/eval_dataset.jsonl\"\n",
    "    shutil.copyfile(\"ibo_eval_dataset.jsonl\", eval_dataset)\n",
    "\n",
    "    eval_dataset = \"/content/drive/MyDrive/Side Projects/NaijEmbeddings/datasets/hausa/eval_dataset.jsonl\"\n",
    "    shutil.copyfile(\"hau_eval_dataset.jsonl\", eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb4d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "def text_to_guid(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic GUID (UUID v5) from a given text.\n",
    "    \"\"\"\n",
    "    namespace = uuid.NAMESPACE_DNS  # Standard namespace, or use your own UUID\n",
    "    return str(uuid.uuid5(namespace, text))\n",
    "\n",
    "\n",
    "def format_evaluation_jsonl(filepath):\n",
    "    filepath = Path(filepath)\n",
    "    lines = []\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        for obj in reader:\n",
    "            lines.append(obj)\n",
    "\n",
    "\n",
    "    dataset = {\"queries\": {}, \"corpus\": {}, \"relevant_docs\": {}, \"mode\": \"text\"}\n",
    "\n",
    "    for line in lines:\n",
    "        query_id = text_to_guid(line[\"query\"])\n",
    "        if isinstance(line[\"pos\"], str):\n",
    "            pos = line[\"pos\"]\n",
    "        elif isinstance(line[\"pos\"], list):\n",
    "            pos = line[\"pos\"][0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type for 'pos': {type(line['pos'])}. Expected a list or string.\")\n",
    "\n",
    "        pos_id = text_to_guid(pos)\n",
    "        dataset[\"queries\"][query_id] = line[\"query\"]\n",
    "        dataset[\"corpus\"][pos_id] = pos\n",
    "        dataset[\"relevant_docs\"][query_id] = [pos_id]\n",
    "\n",
    "    new_path = filepath.parent / (filepath.stem + \"_formatted.json\")\n",
    "    with open(new_path, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92e07665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = split_wura_validation_all_langs()\n",
    "\n",
    "# !cp hau_eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/hausa/eval_dataset.jsonl\n",
    "# !cp hau_test_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/hausa/test_dataset.jsonl\n",
    "\n",
    "# !cp igbo_eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/igbo/eval_dataset.jsonl\n",
    "# !cp igbo_test_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/igbo/test_dataset.jsonl\n",
    "\n",
    "# !cp yor_eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/yoruba/eval_dataset.jsonl\n",
    "# !cp yor_test_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/static_wura/yoruba/test_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b669c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dataset_v3(make_yoruba_df(), filename=\"yor_train_dataset.jsonl\")\n",
    "#make_dataset_v3(make_igbo_df(), filename=\"igbo_train_dataset.jsonl\")\n",
    "#make_dataset_v3(make_hausa_df(), filename=\"hausa_train_dataset.jsonl\")\n",
    "# format_evaluation_jsonl(\"eval_dataset.jsonl\")\n",
    "\n",
    "# OR\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "# !gdown https://drive.google.com/uc?id=1xJ6EHSyaZeMtosQ7RF_R9OHJssuXl0Eq\n",
    "\n",
    "# !gdown https://drive.google.com/uc?id=1qR1n_kb5mtCfbAPitw3bffRKQtN0H-ZL\n",
    "\n",
    "\n",
    "# !gdown https://drive.google.com/uc?id=10RHg1qWjopgjo0Ns0TZ53zhhAmVuO6u4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bffe08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-9s4lsREcIemnva5yMzsTng2cwyvUk0n\n",
      "From (redirected): https://drive.google.com/uc?id=1-9s4lsREcIemnva5yMzsTng2cwyvUk0n&confirm=t&uuid=4b646be8-cd18-4ca9-b0df-a7a2f47c808b\n",
      "To: /home/omotoso.abdulmatin4/filtered_english_train_dataset.jsonl\n",
      "100%|█████████████████████████████████████████| 706M/706M [00:06<00:00, 108MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1-9s4lsREcIemnva5yMzsTng2cwyvUk0n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/hausa_train_dataset.jsonl .\n",
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/igbo_train_dataset.jsonl .\n",
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/yoruba_train_dataset.jsonl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with jsonlines.open(\"yoruba_train_dataset.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)\n",
    "\n",
    "with jsonlines.open(\"hausa_train_dataset.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)\n",
    "\n",
    "with jsonlines.open(\"igbo_train_dataset.jsonl\") as reader:\n",
    "    for obj in reader:\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [10_000, 50_000, 100_000, 300_000]\n",
    "\n",
    "for size in sizes:\n",
    "    with jsonlines.open(f\"{size}_train_dataset.jsonl\", \"w\") as writer:\n",
    "        writer.write_all(data[:size])\n",
    "\n",
    "# 10k\n",
    "gdown https://drive.google.com/uc?id=1qR1n_kb5mtCfbAPitw3bffRKQtN0H-ZL\n",
    "# 50k\n",
    "gdown https://drive.google.com/uc?id=1-2UiPWc6Z0Qn0coN1yYIgOSciNFcEncB\n",
    "# 100k\n",
    "gdown https://drive.google.com/uc?id=1-4WNTv69iQR528_lS7iKQxo24jkBnECr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp 10000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/10000_train_dataset.jsonl\n",
    "!cp 50000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/50000_train_dataset.jsonl\n",
    "!cp 100000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/100000_train_dataset.jsonl\n",
    "!cp 300000_train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combine_wura_all_langs/300000_train_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/combine_wura_all_langs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6036dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/hausa_mato_81k.tsv .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/igbo_mato_3k.tsv .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/alaroye_mato_10k.tsv .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/original_datasets/von_mato_6k.tsv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07694b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets\n",
    "# !cp eval_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets\n",
    "# !cp train_dataset.jsonl /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/igbo_comwura/\n",
    "!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/hausa/eval_dataset.jsonl .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/hausa_igbo_comwura/train_dataset.jsonl .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/combined_wura/eval_dataset.jsonl .\n",
    "# !cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/datasets/test_dataset.jsonl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_evaluation_jsonl(\"eval_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a60a5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-21 18:54:46--  https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/refs/heads/master/examples/finetune/ds_stage0.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 963 [text/plain]\n",
      "Saving to: ‘ds_stage0.json.4’\n",
      "\n",
      "ds_stage0.json.4    100%[===================>]     963  --.-KB/s    in 0s      \n",
      "\n",
      "2025-04-21 18:54:46 (105 MB/s) - ‘ds_stage0.json.4’ saved [963/963]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/FlagOpen/FlagEmbedding/refs/heads/master/examples/finetune/ds_stage0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f1126a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined and shuffled 180530 entries into 'combined_english_shuffled_dataset.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Input files\n",
    "input_files = [\n",
    "    '/home/omotoso.abdulmatin4/filtered_hausa_train_dataset.jsonl',\n",
    "    '/home/omotoso.abdulmatin4/filtered_igbo_train_dataset.jsonl',\n",
    "    '/home/omotoso.abdulmatin4/filtered_yoruba_train_dataset.jsonl',\n",
    "    '/home/omotoso.abdulmatin4/filtered_english_train_dataset.jsonl'\n",
    "]\n",
    "\n",
    "# Output file\n",
    "output_file = 'combined_english_shuffled_dataset.jsonl'\n",
    "\n",
    "# Read and combine all lines\n",
    "all_data = []\n",
    "for file_path in input_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            all_data.append(json.loads(line.strip()))\n",
    "\n",
    "# Shuffle the combined data\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# Write the shuffled data to the output file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Combined and shuffled {len(all_data)} entries into '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17fb8558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torchrun --standalone --nproc_per_node 8 -m FlagEmbedding.finetune.embedder.encoder_only.m3 --model_name_or_path BAAI/bge-m3 --output_dir ./bge-m3 --cache_dir ./cache/model --cache_path ./cache/data --train_data /home/omotoso.abdulmatin4/combined_english_shuffled_dataset.jsonl --trust_remote_code True --train_group_size 2 --query_max_len 512 --passage_max_len 2048 --overwrite_output_dir --learning_rate 1e-5 --fp16 --dataloader_num_workers 12 --gradient_checkpointing --deepspeed ds_stage0.json --num_train_epochs 3 --per_device_train_batch_size 8 --dataloader_drop_last False --warmup_ratio 0.1 --report_to none --logging_steps 100 --save_steps 500 --temperature 0.01 --sentence_pooling_method cls --normalize_embeddings True --knowledge_distillation False --kd_loss_type m3_kd_loss --unified_finetuning False --use_self_distill False --fix_encoder False\n"
     ]
    }
   ],
   "source": [
    "# # Train a model, terminal command\n",
    "import re\n",
    "\n",
    "command = \"\"\"\n",
    "torchrun --standalone --nproc_per_node 8 \\\n",
    "-m FlagEmbedding.finetune.embedder.encoder_only.m3 \\\n",
    "--model_name_or_path BAAI/bge-m3 \\\n",
    "--output_dir ./bge-m3 \\\n",
    "--cache_dir ./cache/model \\\n",
    "--cache_path ./cache/data \\\n",
    "--train_data /home/omotoso.abdulmatin4/combined_english_shuffled_dataset.jsonl \\\n",
    "--trust_remote_code True \\\n",
    "--train_group_size 2 \\\n",
    "--query_max_len 512 \\\n",
    "--passage_max_len 2048 \\\n",
    "--overwrite_output_dir \\\n",
    "--learning_rate 1e-5 \\\n",
    "--fp16 \\\n",
    "--dataloader_num_workers 12 \\\n",
    "--gradient_checkpointing \\\n",
    "--deepspeed ds_stage0.json \\\n",
    "--num_train_epochs 3 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--dataloader_drop_last False \\\n",
    "--warmup_ratio 0.1 \\\n",
    "--report_to none \\\n",
    "--logging_steps 100 \\\n",
    "--save_steps 500 \\\n",
    "--temperature 0.01 \\\n",
    "--sentence_pooling_method cls \\\n",
    "--normalize_embeddings True \\\n",
    "--knowledge_distillation False \\\n",
    "--kd_loss_type m3_kd_loss \\\n",
    "--unified_finetuning False \\\n",
    "--use_self_distill False \\\n",
    "--fix_encoder False\"\"\"\n",
    "\n",
    "command = re.sub(r'\\\\\\n\\s+', '', command)\n",
    "\n",
    "print(command)\n",
    "\n",
    "# OR\n",
    "\n",
    "# Download existing model weights\n",
    "# !gdown https://drive.google.com/uc?id=1hC2nReprpHpCNWq9yergzGJLSHz_VKia\n",
    "# !tar -xzvf bge-m3-5-epochs-unified.tar.gz\n",
    "\n",
    "#gdown https://drive.google.com/uc?id=1-2UiPWc6Z0Qn0coN1yYIgOSciNFcEncB\n",
    "#gdown https://drive.google.com/uc?id=1-4WNTv69iQR528_lS7iKQxo24jkBnECr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee9b1866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0505 07:29:57.464000 167865 torch/distributed/run.py:792] \n",
      "W0505 07:29:57.464000 167865 torch/distributed/run.py:792] *****************************************\n",
      "W0505 07:29:57.464000 167865 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0505 07:29:57.464000 167865 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-05 07:30:03,362] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,364] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,366] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,470] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,571] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,581] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,631] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:03,749] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 07:30:05,371] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-05 07:30:05,376] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-05 07:30:05,431] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-05 07:30:05,580] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-05 07:30:05,784] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-05 07:30:05,791] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-05 07:30:05,791] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-05 07:30:05,796] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/05/2025 07:30:06 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Training/evaluation parameters EncoderOnlyEmbedderM3TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=12,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=ds_stage0.json,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "fix_encoder=False,\n",
      "fix_position_embedding=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "kd_loss_type=m3_kd_loss,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./bge-m3/runs/May05_07-30-05_yoruba-embedding,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "negatives_cross_device=False,\n",
      "no_cuda=False,\n",
      "normalize_embeddings=True,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./bge-m3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./bge-m3,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "self_distill_start_step=-1,\n",
      "sentence_pooling_method=cls,\n",
      "skip_memory_metrics=True,\n",
      "sub_batch_size=None,\n",
      "temperature=0.01,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "unified_finetuning=False,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_self_distill=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "05/05/2025 07:30:06 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Model parameters EncoderOnlyEmbedderM3ModelArguments(model_name_or_path='BAAI/bge-m3', config_name=None, tokenizer_name=None, cache_dir='./cache/model', trust_remote_code=True, token=None, colbert_dim=-1)\n",
      "05/05/2025 07:30:06 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Data parameters AbsEmbedderDataArguments(train_data=['/home/omotoso.abdulmatin4/combined_english_shuffled_dataset.jsonl'], cache_path='./cache/data', train_group_size=2, query_max_len=512, passage_max_len=2048, pad_to_multiple_of=None, max_example_num_per_dataset=100000000, query_instruction_for_retrieval=None, query_instruction_format='{}{}', knowledge_distillation=False, passage_instruction_for_retrieval=None, passage_instruction_format='{}{}', shuffle_ratio=0.0, same_dataset_within_batch=False, small_threshold=0, drop_threshold=0)\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "[2025-05-05 07:30:06,223] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "tokenizer_config.json: 100%|███████████████████| 444/444 [00:00<00:00, 2.78MB/s]\n",
      "sentencepiece.bpe.model:   0%|                      | 0.00/5.07M [00:00<?, ?B/s]05/05/2025 07:30:06 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "sentencepiece.bpe.model: 100%|█████████████| 5.07M/5.07M [00:00<00:00, 60.4MB/s]\n",
      "tokenizer.json: 100%|███████████████████████| 17.1M/17.1M [00:00<00:00, 208MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 964/964 [00:00<00:00, 6.38MB/s]\n",
      "config.json: 100%|█████████████████████████████| 687/687 [00:00<00:00, 4.62MB/s]\n",
      "05/05/2025 07:30:08 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.runner -   Config: XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 85948.85it/s]\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 199728.76it/s]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 48657.82it/s]\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 188366.95it/s]\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 178228.22it/s]\n",
      "Fetching 30 files: 100%|█████████████████████| 30/30 [00:00<00:00, 84335.87it/s]\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 169809.88it/s]\n",
      "Fetching 30 files: 100%|████████████████████| 30/30 [00:00<00:00, 145299.21it/s]\n",
      "05/05/2025 07:30:09 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.runner -   loading existing colbert_linear and sparse_linear---------\n",
      "05/05/2025 07:30:09 - INFO - FlagEmbedding.abc.finetune.embedder.AbsDataset -   loading data from /home/omotoso.abdulmatin4/combined_english_shuffled_dataset.jsonl ...\n",
      "Generating train split: 180530 examples [00:14, 12045.71 examples/s]\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderM3Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderM3Trainer(\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Using /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/omotoso.abdulmatin4/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05093050003051758 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1018674373626709 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10156536102294922 seconds\n",
      "Time to load fused_adam op: 0.10178613662719727 seconds\n",
      "Time to load fused_adam op: 0.10182070732116699 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1019742488861084 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10268831253051758 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1022493839263916 seconds\n",
      "[2025-05-05 07:30:27,199] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,200] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,200] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,201] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,202] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,203] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,203] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-05 07:30:27,205] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "  0%|                                                  | 0/8463 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1967, 'grad_norm': 23.732534953772454, 'learning_rate': 6.815965374700973e-06, 'epoch': 0.04}\n",
      "{'loss': 0.1333, 'grad_norm': 14.815011757142072, 'learning_rate': 7.844114223121067e-06, 'epoch': 0.07}\n",
      "{'loss': 0.0991, 'grad_norm': 15.019880217825058, 'learning_rate': 8.450528649283854e-06, 'epoch': 0.11}\n",
      "{'loss': 0.1048, 'grad_norm': 16.169684898500552, 'learning_rate': 8.879735657848579e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0943, 'grad_norm': 12.69942000859439, 'learning_rate': 9.212215668297609e-06, 'epoch': 0.18}\n",
      "  6%|██▏                                  | 500/8463 [40:35<10:45:40,  4.87s/it]05/05/2025 08:11:03 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0885, 'grad_norm': 0.711634409625335, 'learning_rate': 9.483646699151794e-06, 'epoch': 0.21}\n",
      "{'loss': 0.079, 'grad_norm': 2.0333328132420276, 'learning_rate': 9.713007739956234e-06, 'epoch': 0.25}\n",
      "{'loss': 0.0918, 'grad_norm': 9.446517980998715, 'learning_rate': 9.911606729664642e-06, 'epoch': 0.28}\n",
      "{'loss': 0.0608, 'grad_norm': 1.8436358141697364, 'learning_rate': 9.9343487394958e-06, 'epoch': 0.32}\n",
      "{'loss': 0.0719, 'grad_norm': 4.5629406023876715, 'learning_rate': 9.803046218487395e-06, 'epoch': 0.35}\n",
      " 12%|████                              | 1000/8463 [1:21:13<10:08:07,  4.89s/it]05/05/2025 08:51:41 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-1000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0683, 'grad_norm': 1.0829478764094393, 'learning_rate': 9.673056722689077e-06, 'epoch': 0.39}\n",
      "{'loss': 0.053, 'grad_norm': 1.0093300210007305, 'learning_rate': 9.541754201680672e-06, 'epoch': 0.43}\n",
      "{'loss': 0.0678, 'grad_norm': 10.90459265590471, 'learning_rate': 9.41045168067227e-06, 'epoch': 0.46}\n",
      "{'loss': 0.0651, 'grad_norm': 7.280890852948479, 'learning_rate': 9.279149159663865e-06, 'epoch': 0.5}\n",
      "{'loss': 0.0642, 'grad_norm': 11.006258138017825, 'learning_rate': 9.147846638655463e-06, 'epoch': 0.53}\n",
      " 18%|██████▏                            | 1500/8463 [2:01:46<9:23:12,  4.85s/it]05/05/2025 09:32:14 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-1500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0707, 'grad_norm': 10.320750132659024, 'learning_rate': 9.016544117647058e-06, 'epoch': 0.57}\n",
      "{'loss': 0.0492, 'grad_norm': 5.632455758955139, 'learning_rate': 8.885241596638656e-06, 'epoch': 0.6}\n",
      "{'loss': 0.0573, 'grad_norm': 5.113909747589816, 'learning_rate': 8.753939075630253e-06, 'epoch': 0.64}\n",
      "{'loss': 0.0565, 'grad_norm': 11.490621308343506, 'learning_rate': 8.622636554621849e-06, 'epoch': 0.67}\n",
      "{'loss': 0.0579, 'grad_norm': 4.728885098896404, 'learning_rate': 8.491334033613446e-06, 'epoch': 0.71}\n",
      " 24%|████████▎                          | 2000/8463 [2:42:16<8:39:15,  4.82s/it]05/05/2025 10:12:44 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-2000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0517, 'grad_norm': 7.58976986663087, 'learning_rate': 8.361344537815128e-06, 'epoch': 0.74}\n",
      "{'loss': 0.048, 'grad_norm': 1.903170239642153, 'learning_rate': 8.230042016806723e-06, 'epoch': 0.78}\n",
      "{'loss': 0.0598, 'grad_norm': 10.62078845759285, 'learning_rate': 8.09873949579832e-06, 'epoch': 0.82}\n",
      "{'loss': 0.0478, 'grad_norm': 0.8850961366797769, 'learning_rate': 7.967436974789916e-06, 'epoch': 0.85}\n",
      "{'loss': 0.0562, 'grad_norm': 9.311528302387954, 'learning_rate': 7.836134453781514e-06, 'epoch': 0.89}\n",
      " 30%|██████████▎                        | 2500/8463 [3:22:55<7:56:50,  4.80s/it]05/05/2025 10:53:23 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-2500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0476, 'grad_norm': 4.326967160253034, 'learning_rate': 7.706144957983194e-06, 'epoch': 0.92}\n",
      "{'loss': 0.0414, 'grad_norm': 7.556480344885224, 'learning_rate': 7.574842436974791e-06, 'epoch': 0.96}\n",
      "{'loss': 0.0398, 'grad_norm': 12.618684605452147, 'learning_rate': 7.443539915966387e-06, 'epoch': 0.99}\n",
      " 33%|███████████▋                       | 2821/8463 [3:49:02<7:50:33,  5.00s/it]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0418, 'grad_norm': 1.9255874257691847, 'learning_rate': 7.312237394957984e-06, 'epoch': 1.03}\n",
      "{'loss': 0.0266, 'grad_norm': 0.04033409862669302, 'learning_rate': 7.18093487394958e-06, 'epoch': 1.06}\n",
      " 35%|████████████▍                      | 3000/8463 [4:03:28<7:24:24,  4.88s/it]05/05/2025 11:33:56 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-3000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0343, 'grad_norm': 2.687798771773815, 'learning_rate': 7.049632352941177e-06, 'epoch': 1.1}\n",
      "{'loss': 0.0314, 'grad_norm': 1.3367473980471827, 'learning_rate': 6.9183298319327745e-06, 'epoch': 1.13}\n",
      "{'loss': 0.0295, 'grad_norm': 0.03040771737054656, 'learning_rate': 6.78702731092437e-06, 'epoch': 1.17}\n",
      "{'loss': 0.0307, 'grad_norm': 0.05722455022808768, 'learning_rate': 6.6557247899159676e-06, 'epoch': 1.21}\n",
      "{'loss': 0.0219, 'grad_norm': 6.890564379782242, 'learning_rate': 6.524422268907563e-06, 'epoch': 1.24}\n",
      " 41%|██████████████▍                    | 3500/8463 [4:43:55<6:40:00,  4.84s/it]05/05/2025 12:14:22 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-3500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0289, 'grad_norm': 4.086316997344088, 'learning_rate': 6.393119747899161e-06, 'epoch': 1.28}\n",
      "{'loss': 0.0261, 'grad_norm': 3.996234432663843, 'learning_rate': 6.261817226890757e-06, 'epoch': 1.31}\n",
      "{'loss': 0.0397, 'grad_norm': 1.2808812704073356, 'learning_rate': 6.131827731092438e-06, 'epoch': 1.35}\n",
      "{'loss': 0.0263, 'grad_norm': 1.9168505856825202, 'learning_rate': 6.000525210084034e-06, 'epoch': 1.38}\n",
      "{'loss': 0.0359, 'grad_norm': 5.052362912756492, 'learning_rate': 5.869222689075631e-06, 'epoch': 1.42}\n",
      " 47%|████████████████▌                  | 4000/8463 [5:24:21<5:52:02,  4.73s/it]05/05/2025 12:54:48 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-4000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0321, 'grad_norm': 7.269321871145107, 'learning_rate': 5.737920168067227e-06, 'epoch': 1.45}\n",
      "{'loss': 0.0277, 'grad_norm': 7.313138493759451, 'learning_rate': 5.606617647058824e-06, 'epoch': 1.49}\n",
      "{'loss': 0.0348, 'grad_norm': 0.12276908003829656, 'learning_rate': 5.475315126050421e-06, 'epoch': 1.52}\n",
      "{'loss': 0.0202, 'grad_norm': 10.74629218453193, 'learning_rate': 5.344012605042017e-06, 'epoch': 1.56}\n",
      "{'loss': 0.0245, 'grad_norm': 6.04146110798836, 'learning_rate': 5.2140231092436976e-06, 'epoch': 1.6}\n",
      " 53%|██████████████████▌                | 4500/8463 [6:04:44<5:21:15,  4.86s/it]05/05/2025 13:35:12 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-4500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0277, 'grad_norm': 1.593987091979026, 'learning_rate': 5.082720588235295e-06, 'epoch': 1.63}\n",
      "{'loss': 0.0289, 'grad_norm': 0.9999676937606852, 'learning_rate': 4.9514180672268915e-06, 'epoch': 1.67}\n",
      "{'loss': 0.0219, 'grad_norm': 3.147079288401446, 'learning_rate': 4.820115546218488e-06, 'epoch': 1.7}\n",
      "{'loss': 0.0331, 'grad_norm': 6.504718828274756, 'learning_rate': 4.6888130252100845e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0252, 'grad_norm': 0.5230125866135782, 'learning_rate': 4.557510504201681e-06, 'epoch': 1.77}\n",
      " 59%|████████████████████▋              | 5000/8463 [6:45:08<4:36:39,  4.79s/it]05/05/2025 14:15:36 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-5000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0219, 'grad_norm': 1.5859762102485924, 'learning_rate': 4.426207983193278e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0235, 'grad_norm': 9.639866037031535, 'learning_rate': 4.294905462184874e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0255, 'grad_norm': 0.8851111539445405, 'learning_rate': 4.1636029411764715e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0259, 'grad_norm': 0.18041942632501295, 'learning_rate': 4.032300420168068e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0332, 'grad_norm': 2.561537957450269, 'learning_rate': 3.9009978991596646e-06, 'epoch': 1.95}\n",
      " 65%|██████████████████████▋            | 5500/8463 [7:25:29<3:59:26,  4.85s/it]05/05/2025 14:55:57 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-5500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0268, 'grad_norm': 1.7674203032961395, 'learning_rate': 3.7696953781512607e-06, 'epoch': 1.99}\n",
      " 67%|███████████████████████▎           | 5642/8463 [7:37:08<3:50:56,  4.91s/it]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0257, 'grad_norm': 20.896530286804644, 'learning_rate': 3.638392857142857e-06, 'epoch': 2.02}\n",
      "{'loss': 0.0179, 'grad_norm': 5.487384372613934, 'learning_rate': 3.5070903361344537e-06, 'epoch': 2.06}\n",
      "{'loss': 0.0183, 'grad_norm': 2.1647584532856725, 'learning_rate': 3.375787815126051e-06, 'epoch': 2.09}\n",
      "{'loss': 0.0223, 'grad_norm': 3.0735523320724965, 'learning_rate': 3.2444852941176476e-06, 'epoch': 2.13}\n",
      " 71%|████████████████████████▊          | 6000/8463 [8:05:52<3:17:21,  4.81s/it]05/05/2025 15:36:21 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-6000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.024, 'grad_norm': 2.257284663714249, 'learning_rate': 3.113182773109244e-06, 'epoch': 2.16}\n",
      "{'loss': 0.0133, 'grad_norm': 3.8278422348533065, 'learning_rate': 2.9818802521008407e-06, 'epoch': 2.2}\n",
      "{'loss': 0.0112, 'grad_norm': 0.009143376567963102, 'learning_rate': 2.850577731092437e-06, 'epoch': 2.23}\n",
      "{'loss': 0.0227, 'grad_norm': 0.03554660101407696, 'learning_rate': 2.7192752100840337e-06, 'epoch': 2.27}\n",
      "{'loss': 0.023, 'grad_norm': 1.6976896575014102, 'learning_rate': 2.5879726890756307e-06, 'epoch': 2.3}\n",
      " 77%|██████████████████████████▉        | 6500/8463 [8:46:13<2:37:44,  4.82s/it]05/05/2025 16:16:41 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-6500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0157, 'grad_norm': 0.4331752888548042, 'learning_rate': 2.456670168067227e-06, 'epoch': 2.34}\n",
      "{'loss': 0.0157, 'grad_norm': 1.7437374593085535, 'learning_rate': 2.3253676470588237e-06, 'epoch': 2.38}\n",
      "{'loss': 0.0151, 'grad_norm': 0.24671097207447404, 'learning_rate': 2.1940651260504203e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0159, 'grad_norm': 1.0770856297737201, 'learning_rate': 2.062762605042017e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0188, 'grad_norm': 11.838440200543142, 'learning_rate': 1.9314600840336138e-06, 'epoch': 2.48}\n",
      " 83%|████████████████████████████▉      | 7000/8463 [9:26:33<1:56:22,  4.77s/it]05/05/2025 16:57:01 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-7000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0129, 'grad_norm': 0.026158439199734863, 'learning_rate': 1.8001575630252103e-06, 'epoch': 2.52}\n",
      "{'loss': 0.0123, 'grad_norm': 0.06030359537845879, 'learning_rate': 1.6688550420168068e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0148, 'grad_norm': 0.40158364800602275, 'learning_rate': 1.5375525210084036e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0194, 'grad_norm': 0.012387739381798232, 'learning_rate': 1.40625e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0141, 'grad_norm': 0.1309883563803916, 'learning_rate': 1.2749474789915966e-06, 'epoch': 2.66}\n",
      " 89%|██████████████████████████████▏   | 7500/8463 [10:06:57<1:17:02,  4.80s/it]05/05/2025 17:37:25 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-7500\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0161, 'grad_norm': 9.37668116436744, 'learning_rate': 1.1436449579831934e-06, 'epoch': 2.69}\n",
      "{'loss': 0.013, 'grad_norm': 13.566842082871108, 'learning_rate': 1.01234243697479e-06, 'epoch': 2.73}\n",
      "{'loss': 0.015, 'grad_norm': 11.464046501241052, 'learning_rate': 8.810399159663867e-07, 'epoch': 2.76}\n",
      "{'loss': 0.0151, 'grad_norm': 2.8272717279356057, 'learning_rate': 7.497373949579833e-07, 'epoch': 2.8}\n",
      "{'loss': 0.0151, 'grad_norm': 1.4613860558572982, 'learning_rate': 6.184348739495799e-07, 'epoch': 2.84}\n",
      " 95%|██████████████████████████████████  | 8000/8463 [10:47:19<37:11,  4.82s/it]05/05/2025 18:17:46 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-8000\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'loss': 0.0216, 'grad_norm': 10.268636740711754, 'learning_rate': 4.884453781512605e-07, 'epoch': 2.87}\n",
      "{'loss': 0.0144, 'grad_norm': 0.009449713907158997, 'learning_rate': 3.5714285714285716e-07, 'epoch': 2.91}\n",
      "{'loss': 0.0164, 'grad_norm': 0.31855925621624975, 'learning_rate': 2.258403361344538e-07, 'epoch': 2.94}\n",
      "{'loss': 0.0124, 'grad_norm': 0.07650944601340874, 'learning_rate': 9.453781512605043e-08, 'epoch': 2.98}\n",
      "100%|████████████████████████████████████| 8463/8463 [11:24:42<00:00,  4.93s/it]05/05/2025 18:55:10 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3/checkpoint-8463\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "{'train_runtime': 41097.4981, 'train_samples_per_second': 13.178, 'train_steps_per_second': 0.206, 'train_loss': 0.03904583997708411, 'epoch': 3.0}\n",
      "100%|████████████████████████████████████| 8463/8463 [11:24:57<00:00,  4.86s/it]\n",
      "05/05/2025 18:55:25 - INFO - FlagEmbedding.finetune.embedder.encoder_only.m3.trainer -   Saving model checkpoint to ./bge-m3\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "[rank0]:[W505 18:55:27.802405430 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940137ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omotoso.abdulmatin4/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa60c4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bge-m3/\n",
      "./bge-m3/tokenizer_config.json\n",
      "./bge-m3/model.safetensors\n",
      "./bge-m3/tokenizer.json\n",
      "./bge-m3/sentencepiece.bpe.model\n",
      "./bge-m3/config.json\n",
      "./bge-m3/training_args.bin\n",
      "./bge-m3/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#model_id = \"bge-m3-yoruba-igbo-hausa-alldatatogether-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize\"\n",
    "#!tar --exclude='global_*' -czvf {model_id}.tar.gz ./bge-m3/checkpoint-3000\n",
    "#!cp {model_id}.tar.gz /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/experiments/model_weights/\n",
    "\n",
    "model_id = \"bge-m3-yoruba-igbo-hausa-english-alldatatogether-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize\"\n",
    "!tar --exclude='./bge-m3/checkpoint-*' -czvf {model_id}.tar.gz ./bge-m3\n",
    "#!cp {model_id}.tar.gz /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/experiments/model_weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a513229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bge-m3/\n",
      "./bge-m3/tokenizer_config.json\n",
      "./bge-m3/model.safetensors\n",
      "./bge-m3/tokenizer.json\n",
      "./bge-m3/sentencepiece.bpe.model\n",
      "./bge-m3/config.json\n",
      "./bge-m3/training_args.bin\n",
      "./bge-m3/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"bge-m3-yoruba-igbo-hausa-english-alldatatogether-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize\",\n",
    "\n",
    "]\n",
    "\n",
    "for id_ in model_ids:\n",
    "    #!cp /content/drive/MyDrive/Side\\ Projects/NaijEmbeddings/experiments/model_weights/{id_}.tar.gz .\n",
    "    !mkdir {id_}\n",
    "    !tar -xzvf {id_}.tar.gz -C {id_}\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491318b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-08 10:44:35,208] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import util\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from FlagEmbedding.inference.embedder.encoder_only.m3 import M3Embedder\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b196341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output):\n",
    "    return torch.mean(model_output[\"last_hidden_state\"], dim=1)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "def last_token_pooling(model_output):\n",
    "    return model_output[0][:, -1]\n",
    "\n",
    "def get_sentence_embedding(text, tokenizer, embed_model, normalize, max_length, pooling_type='cls'):\n",
    "\n",
    "    if pooling_type==\"last_token\":\n",
    "        encoded_input = tokenizer(text, max_length=max_length, return_attention_mask=False, padding=False, truncation=True)\n",
    "        encoded_input['input_ids'] = encoded_input['input_ids'] + [tokenizer.eos_token_id]\n",
    "        encoded_input = tokenizer.pad([encoded_input], padding=True, return_attention_mask=True, return_tensors='pt').to(\"cuda\")\n",
    "    else:\n",
    "        encoded_input = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = embed_model(**encoded_input)\n",
    "\n",
    "    if pooling_type==\"cls\":\n",
    "        sentence_embeddings = cls_pooling(model_output)\n",
    "    if pooling_type==\"mean\":\n",
    "        sentence_embeddings = mean_pooling(model_output)\n",
    "    if pooling_type==\"last_token\":\n",
    "        sentence_embeddings = last_token_pooling(model_output)\n",
    "\n",
    "    if normalize:\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542f6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def truncate_text(text, model=\"text-embedding-3-large\", max_tokens=8192):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return enc.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4bee728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-large\", normalize=True):\n",
    "    text = truncate_text(text, model=model, max_tokens=8192)\n",
    "   \n",
    "    client = OpenAI(api_key=\"sk-proj-LdGUWAh6BUL4USMR7b73T3BlbkFJN5dlooT1de77qvfu7FPX\")\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "    embedding = torch.tensor(response.data[0].embedding)\n",
    "\n",
    "    if normalize:\n",
    "        embedding = torch.nn.functional.normalize(embedding.unsqueeze(0), p=2, dim=1)\n",
    "    else:\n",
    "        embedding = embedding.unsqueeze(0)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c051887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qa_dataset, tokenizer, model_name, embed_model, normalize, max_length=None, pooling_type=\"cls\", top_k=5, verbose=False):\n",
    "    input_texts = list(qa_dataset.corpus.values())\n",
    "    input_text_keys = list(qa_dataset.corpus.keys())\n",
    "    queries = qa_dataset.queries\n",
    "    relevant_docs = qa_dataset.relevant_docs\n",
    "\n",
    "    embeddings = []\n",
    "    for sentence in input_texts:\n",
    "        if model_name == \"intfloat/multilingual-e5-large\":\n",
    "            sentence = f\"passage: {sentence}\"\n",
    "\n",
    "        if embed_model == \"openai\":\n",
    "            embedding = get_openai_embedding(sentence, model=model_name, normalize=normalize)\n",
    "        else:\n",
    "            embedding = get_sentence_embedding(sentence, tokenizer, embed_model, normalize, max_length, pooling_type)\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    embeddings = torch.cat(embeddings)\n",
    "\n",
    "    eval_results = []\n",
    "\n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        if model_name == \"intfloat/multilingual-e5-large\":\n",
    "            query = f\"query: {query}\"\n",
    "\n",
    "        if embed_model == \"openai\":\n",
    "            query_embedding = get_openai_embedding(query, model=model_name, normalize=normalize)\n",
    "        else:\n",
    "            query_embedding = get_sentence_embedding(query, tokenizer, embed_model, normalize, max_length, pooling_type)\n",
    "\n",
    "        results = util.semantic_search(query_embedding, embeddings, top_k=top_k)[0]\n",
    "        retrieved_ids = [input_text_keys[int(result[\"corpus_id\"])] for result in results]\n",
    "\n",
    "        expected_id = relevant_docs[query_id][0]\n",
    "        is_hit = expected_id in retrieved_ids\n",
    "\n",
    "        if is_hit:\n",
    "            rank = retrieved_ids.index(expected_id) + 1\n",
    "            mrr = 1 / rank\n",
    "        else:\n",
    "            mrr = 0\n",
    "        eval_results.append(mrr)\n",
    "\n",
    "    return np.average(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26137d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_spec = {\n",
    "}\n",
    "\n",
    "embeddings_model_spec['ML-E5-large']={'model_name':'intfloat/multilingual-e5-large','max_length':512, 'pooling_type':'mean',\n",
    "                                      'normalize': True, 'batch_size':8, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
    "embeddings_model_spec['BGE-M3']={'model_name':'BAAI/bge-m3','max_length':8192, 'pooling_type':'cls', 'vector_type': 'multi-vector',\n",
    "                                 'normalize': True, 'batch_size':8, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
    "embeddings_model_spec['LaBSE']={'model_name':'sentence-transformers/LaBSE','max_length':256, 'pooling_type':'cls',\n",
    "                                 'normalize': True, 'batch_size':8, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
    "#embeddings_model_spec[\"BGE-M3-yoruba-alldata-Epochs-3\"]={'model_name':'bge-m3-yoruba-alldata-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize/bge-m3','max_length':8192, 'pooling_type':'cls', 'vector_type': 'multi-vector',\n",
    "                                 #'normalize': True, 'batch_size':8, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
    "#embeddings_model_spec[\"BGE-M3-yoruba-igbo-alldata-Epochs-3\"]={'model_name':'bge-m3-yoruba-igbo-alldata-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize/bge-m3','max_length':8192, 'pooling_type':'cls', 'vector_type': 'multi-vector',\n",
    "                                 #'normalize': True, 'batch_size':8, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
    "embeddings_model_spec[\"BGE-M3-yoruba-igbo-hausa-english-alldatatogether-Epochs-3\"]={'model_name':'bge-m3-yoruba-igbo-hausa-english-alldatatogether-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize/bge-m3','max_length':8192, 'pooling_type':'cls', 'vector_type': 'multi-vector',\n",
    "                                 'normalize': True, 'batch_size':8, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\n",
    "embeddings_model_spec[\"OpenAI-text-embedding-3-large\"] = {\n",
    "    'model_name': 'text-embedding-3-large',\n",
    "    'max_length': 8192,  # OpenAI has high token limits\n",
    "    'pooling_type': 'cls',  # Not used, just kept for compatibility\n",
    "    'normalize': True,\n",
    "    'batch_size': 1,\n",
    "    'kwargs': {},  # Not used\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f3149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "\n",
    "\n",
    "def text_to_guid(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic GUID (UUID v5) from a given text.\n",
    "    \"\"\"\n",
    "    namespace = uuid.NAMESPACE_DNS  # Standard namespace, or use your own UUID\n",
    "    return str(uuid.uuid5(namespace, text))\n",
    "\n",
    "\n",
    "def format_evaluation_jsonl(filepath):\n",
    "    filepath = Path(filepath)\n",
    "    lines = []\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        for obj in reader:\n",
    "            lines.append(obj)\n",
    "\n",
    "\n",
    "    dataset = {\"queries\": {}, \"corpus\": {}, \"relevant_docs\": {}, \"mode\": \"text\"}\n",
    "\n",
    "    for line in lines:\n",
    "        query_id = text_to_guid(line[\"query\"])\n",
    "        if isinstance(line[\"pos\"], str):\n",
    "            pos = line[\"pos\"]\n",
    "        elif isinstance(line[\"pos\"], list):\n",
    "            pos = line[\"pos\"][0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type for 'pos': {type(line['pos'])}. Expected a list or string.\")\n",
    "\n",
    "        pos_id = text_to_guid(pos)\n",
    "        dataset[\"queries\"][query_id] = line[\"query\"]\n",
    "        dataset[\"corpus\"][pos_id] = pos\n",
    "        dataset[\"relevant_docs\"][query_id] = [pos_id]\n",
    "\n",
    "    new_path = filepath.parent / (filepath.stem + \"_formatted.json\")\n",
    "    with open(new_path, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24528899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19ddg2yh5NPj55-5U8yN1ev-uN6je0lUH\n",
      "To: /home/omotoso.abdulmatin4/Aremu_YO_dataset.json\n",
      "100%|████████████████████████████████████████| 139k/139k [00:00<00:00, 85.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=19ddg2yh5NPj55-5U8yN1ev-uN6je0lUH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc812158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1UDAxYEGOXRLMjEp9me3iAiuKKvUXwlwv\n",
      "To: /home/omotoso.abdulmatin4/english_test_dataset.jsonl\n",
      "100%|███████████████████████████████████████| 47.4M/47.4M [00:00<00:00, 115MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Yoruba\n",
    "!gdown https://drive.google.com/uc?id=1--w3T7vraOUZ3vuD_PF32uLzgq4qh9Ds -O yoruba_test_dataset.jsonl\n",
    "\n",
    "# Igbo\n",
    "!gdown https://drive.google.com/uc?id=1J8FcO2F5h9a64Rb7N3QcVf9wqzaMs23p -O igbo_test_dataset.jsonl\n",
    "\n",
    "# Hausa\n",
    "!gdown https://drive.google.com/uc?id=1-8eWLDk9lWAx-OSE5tjQES89prhcn2yJ -O hausa_test_dataset.jsonl\n",
    "\n",
    "#English\n",
    "!gdown https://drive.google.com/uc?id=1UDAxYEGOXRLMjEp9me3iAiuKKvUXwlwv -O english_test_dataset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ea5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_evaluation_jsonl(\"yoruba_test_dataset.jsonl\")\n",
    "format_evaluation_jsonl(\"igbo_test_dataset.jsonl\")\n",
    "format_evaluation_jsonl(\"hausa_test_dataset.jsonl\")\n",
    "format_evaluation_jsonl(\"english_test_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c441d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML-E5-large\n",
      "Processing model : {'model_name': 'intfloat/multilingual-e5-large', 'max_length': 512, 'pooling_type': 'mean', 'normalize': True, 'batch_size': 8, 'kwargs': {'device_map': 'cuda', 'torch_dtype': torch.float16}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1998/1998 [00:31<00:00, 64.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3\n",
      "Processing model : {'model_name': 'BAAI/bge-m3', 'max_length': 8192, 'pooling_type': 'cls', 'vector_type': 'multi-vector', 'normalize': True, 'batch_size': 8, 'kwargs': {'device_map': 'cuda', 'torch_dtype': torch.float16}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1998/1998 [00:31<00:00, 64.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE\n",
      "Processing model : {'model_name': 'sentence-transformers/LaBSE', 'max_length': 256, 'pooling_type': 'cls', 'normalize': True, 'batch_size': 8, 'kwargs': {'device_map': 'cuda', 'torch_dtype': torch.float16}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1998/1998 [00:16<00:00, 119.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3-yoruba-igbo-hausa-english-alldatatogether-Epochs-3\n",
      "Processing model : {'model_name': 'bge-m3-yoruba-igbo-hausa-english-alldatatogether-3-epochs-e5-lr-0_1-warmup-128-batchsize-0_01-temperature-2-groupsize/bge-m3', 'max_length': 8192, 'pooling_type': 'cls', 'vector_type': 'multi-vector', 'normalize': True, 'batch_size': 8, 'kwargs': {'device_map': 'cuda', 'torch_dtype': torch.float16}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1998/1998 [00:31<00:00, 63.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI-text-embedding-3-large\n",
      "Processing model : {'model_name': 'text-embedding-3-large', 'max_length': 8192, 'pooling_type': 'cls', 'normalize': True, 'batch_size': 1, 'kwargs': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1998/1998 [14:01<00:00,  2.37it/s]  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "#languages = [\"EN\", \"FR\", \"CS\", \"HU\"]\n",
    "\n",
    "for key, model_spec in embeddings_model_spec.items():\n",
    "    print(key)\n",
    "    print(\"Processing model : \"+str(model_spec))\n",
    "\n",
    "    if \"OpenAI\" in key:\n",
    "        tokenizer = None\n",
    "        embed_model = \"openai\"\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_spec['model_name'])\n",
    "        embed_model = AutoModel.from_pretrained(model_spec['model_name'], **model_spec['kwargs'])\n",
    "\n",
    "    file_name = \"/home/omotoso.abdulmatin4/english_test_dataset_formatted.json\"\n",
    "    qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)\n",
    "\n",
    "    start_time_assessment = time.time()\n",
    "    score = evaluate(\n",
    "        qa_dataset,\n",
    "        tokenizer,\n",
    "        model_spec['model_name'],\n",
    "        embed_model,\n",
    "        model_spec['normalize'],\n",
    "        model_spec['max_length'],\n",
    "        model_spec['pooling_type']\n",
    "    )\n",
    "    duration_assessment = time.time() - start_time_assessment\n",
    "    results.append([key, score, duration_assessment])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5182de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Embedding model       MRR    Duration\n",
      "0                                        ML-E5-large  0.676636   37.825116\n",
      "1                                             BGE-M3  0.784685   40.458355\n",
      "2                                              LaBSE  0.320123   20.648884\n",
      "3  BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...  0.920138   40.697949\n",
      "4                      OpenAI-text-embedding-3-large  0.671475  864.519576\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_yoruba = pd.DataFrame(results, columns = [\"Embedding model\", \"MRR\", \"Duration\"])\n",
    "print(df_yoruba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d4f382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Embedding model       MRR    Duration\n",
      "0                                        ML-E5-large  0.679562   26.271745\n",
      "1                                             BGE-M3  0.756658   30.497309\n",
      "2                                              LaBSE  0.300105   14.305604\n",
      "3  BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...  0.863864   30.379917\n",
      "4                      OpenAI-text-embedding-3-large  0.732764  635.219030\n"
     ]
    }
   ],
   "source": [
    "df_igbo = pd.DataFrame(results, columns = [\"Embedding model\", \"MRR\", \"Duration\"])\n",
    "print(df_igbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32d81635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Embedding model       MRR     Duration\n",
      "0                                        ML-E5-large  0.699266    66.460244\n",
      "1                                             BGE-M3  0.857545    76.097345\n",
      "2                                              LaBSE  0.318834    37.391708\n",
      "3  BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...  0.923062    76.355321\n",
      "4                      OpenAI-text-embedding-3-large  0.564999  1687.114425\n"
     ]
    }
   ],
   "source": [
    "df_hausa = pd.DataFrame(results, columns = [\"Embedding model\", \"MRR\", \"Duration\"])\n",
    "print(df_hausa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a66f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Embedding model       MRR     Duration\n",
      "0                                        ML-E5-large  0.352686    67.133896\n",
      "1                                             BGE-M3  0.737788    80.722520\n",
      "2                                              LaBSE  0.434927    37.643577\n",
      "3  BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...  0.861778    80.765280\n",
      "4                      OpenAI-text-embedding-3-large  0.536428  1798.402251\n"
     ]
    }
   ],
   "source": [
    "df_english = pd.DataFrame(results, columns = [\"Embedding model\", \"MRR\", \"Duration\"])\n",
    "print(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61897d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language                                    Embedding model   English  \\\n",
      "0                                                    BGE-M3  0.737788   \n",
      "1         BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...  0.861778   \n",
      "2                                                     LaBSE  0.434927   \n",
      "3                                               ML-E5-large  0.352686   \n",
      "4                             OpenAI-text-embedding-3-large  0.536428   \n",
      "\n",
      "Language     Hausa      Igbo    Yoruba  Macroaverage MRR  \n",
      "0         0.857545  0.756658  0.784685          0.784169  \n",
      "1         0.923062  0.863864  0.920138          0.892211  \n",
      "2         0.318834  0.300105  0.320123          0.343497  \n",
      "3         0.699266  0.679562  0.676636          0.602038  \n",
      "4         0.564999  0.732764  0.671475          0.626417  \n"
     ]
    }
   ],
   "source": [
    "# Add language columns\n",
    "df_yoruba[\"Language\"] = \"Yoruba\"\n",
    "df_igbo[\"Language\"] = \"Igbo\"\n",
    "df_hausa[\"Language\"] = \"Hausa\"\n",
    "df_english[\"Language\"] = \"English\"\n",
    "\n",
    "# Combine all three into one DataFrame\n",
    "df_all = pd.concat([df_yoruba, df_igbo, df_hausa, df_english], ignore_index=True)\n",
    "\n",
    "# Compute macroaverage MRR per model\n",
    "macro_mrr = df_all.groupby(\"Embedding model\")[\"MRR\"].mean().reset_index()\n",
    "macro_mrr.columns = [\"Embedding model\", \"Macroaverage MRR\"]\n",
    "\n",
    "# Pivot to see per-language and macroaverage side-by-side\n",
    "pivot = df_all.pivot_table(index=\"Embedding model\", columns=\"Language\", values=\"MRR\")\n",
    "pivot[\"Macroaverage MRR\"] = macro_mrr.set_index(\"Embedding model\")[\"Macroaverage MRR\"]\n",
    "pivot = pivot.reset_index()\n",
    "\n",
    "# Show result\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eca73874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Language</th>\n",
       "      <th>Embedding model</th>\n",
       "      <th>English</th>\n",
       "      <th>Hausa</th>\n",
       "      <th>Igbo</th>\n",
       "      <th>Yoruba</th>\n",
       "      <th>Macroaverage MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BGE-M3</td>\n",
       "      <td>0.737788</td>\n",
       "      <td>0.857545</td>\n",
       "      <td>0.756658</td>\n",
       "      <td>0.784685</td>\n",
       "      <td>0.784169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...</td>\n",
       "      <td>0.861778</td>\n",
       "      <td>0.923062</td>\n",
       "      <td>0.863864</td>\n",
       "      <td>0.920138</td>\n",
       "      <td>0.892211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LaBSE</td>\n",
       "      <td>0.434927</td>\n",
       "      <td>0.318834</td>\n",
       "      <td>0.300105</td>\n",
       "      <td>0.320123</td>\n",
       "      <td>0.343497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ML-E5-large</td>\n",
       "      <td>0.352686</td>\n",
       "      <td>0.699266</td>\n",
       "      <td>0.679562</td>\n",
       "      <td>0.676636</td>\n",
       "      <td>0.602038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OpenAI-text-embedding-3-large</td>\n",
       "      <td>0.536428</td>\n",
       "      <td>0.564999</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.671475</td>\n",
       "      <td>0.626417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Language                                    Embedding model   English  \\\n",
       "0                                                    BGE-M3  0.737788   \n",
       "1         BGE-M3-yoruba-igbo-hausa-english-alldatatogeth...  0.861778   \n",
       "2                                                     LaBSE  0.434927   \n",
       "3                                               ML-E5-large  0.352686   \n",
       "4                             OpenAI-text-embedding-3-large  0.536428   \n",
       "\n",
       "Language     Hausa      Igbo    Yoruba  Macroaverage MRR  \n",
       "0         0.857545  0.756658  0.784685          0.784169  \n",
       "1         0.923062  0.863864  0.920138          0.892211  \n",
       "2         0.318834  0.300105  0.320123          0.343497  \n",
       "3         0.699266  0.679562  0.676636          0.602038  \n",
       "4         0.564999  0.732764  0.671475          0.626417  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv-env)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
